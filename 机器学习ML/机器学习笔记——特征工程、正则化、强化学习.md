> 大家好，这里是好评笔记，公主号：Goodnote，专栏文章私信限时Free。本笔记介绍机器学习中常见的特征工程方法、正则化方法和简要介绍强化学习。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/16b570199b7b4484a8f44a8a044ed1b0.jpeg#pic_center)


@[toc]
# 特征工程（Fzeature Engineering）
## 1. 特征提取（Feature Extraction）
**特征提取**：从**原始数据**中**提取**能够有效表征**数据特征**的过程。它将原始数据转换为适合模型输入的特征表示。
### 手工特征提取（Manual Feature Extraction）：
  - **文本数据**：
    - **词袋模型**（Bag of Words）：将文本数据转化为**词频向量**，每个单词是一个维度，值为该单词在文本中出现的次数。
    - **TF-IDF**：为词袋模型加入**词频-逆文档频率**（Term Frequency-Inverse Document Frequency），**降低常见词语的权重**，**提升重要词语的权重**。
    - **N-gram**：将连续的 N 个词作为一个特征，捕捉**词语间的局部依赖关系**。
  - **图像数据**：
    - **边缘检测**：使用 Sobel 算子、Canny 边缘检测等方法提取图像边缘信息。
    - **SIFT（尺度不变特征变换）**：提取图像的关键点和局部特征，具有尺度不变性和旋转不变性。
    - **HOG（方向梯度直方图）**：将图像分块，并统计每块的梯度方向直方图，用于描述局部形状和纹理特征。
  - **时间序列数据**：
    - **移动平均**：对时间序列进行平滑，消除短期波动。
    - **傅里叶变换**：将时间域的信号转化为频域信号，分析数据的周期性。
    - **窗口函数**：将时间序列分为若干窗口，分别计算每个窗口的统计特征，如均值、方差等。

### 自动特征提取（Automated Feature Extraction）：
- 使用**卷积神经网络（CNN）**：从图像中自动提取高级特征，如边缘、纹理、形状等。
- 使用**循环神经网络（RNN）**：处理时间序列数据，捕捉长时间依赖关系。
- 使用**BERT（Transformer）**：通过自监督学习自动提取上下文敏感的文本特征。
- **自动编码器（Autoencoder）**：使用无监督学习从数据中提取低维特征表示，捕捉数据的潜在结构和模式。

## 2. 特征选择（Feature Selection）
特征选择（Feature Selection）是指**从原始特征集中挑选出与目标任务最相关的特征**，以提高模型的性能、减少训练时间以及降低过拟合的风险。特征选择方法主要分为三类：**过滤法（Filter Methods）**、**包裹法（Wrapper Methods）** 和 **嵌入法（Embedded Methods）**。
### 1. 过滤法（Filter Methods）
   - **原理**：独立于模型，**训练前**首先根据某些**统计指标对特征进行评分**，然后选择得分较高的特征。这种方法不依赖于特定的学习算法，只是**基于数据本身的特性进行筛选**。
   - **常见方法**：
     - **方差选择法**：**剔除方差较小的特征**，认为方差小的特征对目标值影响小。
     - **皮尔森相关系数**：计算特征与目标变量之间的线性相关性，**选择线性相关性较高的特征**。
     - **互信息**：衡量特征与目标变量之间的**信息增益**，选择信息量大的特征。
   - **优点**：**计算效率高，易于实现**。
   - **缺点**：未考虑特征之间的相互作用，可能遗漏组合特征的重要性。

### 2. 包裹法（Wrapper Methods）
   - **原理**：在**训练中**，通过训练模型**评估特征子集的表现**，使用搜索策略找到对目标任务最优的**特征组合**。包裹法直接**根据模型的性能进行选择**，通常**通过交叉验证**来评估特征子集的好坏。
   - **常见方法**：
     - **前向选择（Forward Selection）**：从**空集开始**，**逐步添加**对模型性能提升最大的特征。
     - **后向消除（Backward Elimination）**：从**所有特征开始**，**逐步移除**对模型性能影响最小的特征。
   - **优点**：能够**考虑特征之间的相互作用**，适合复杂的特征选择任务。
   - **缺点**：计算开销大，尤其是当特征数目较多时，训练多个模型的过程会非常耗时。

### 3. 嵌入法（Embedded Methods）
   - **原理**：嵌入法**结合了过滤法和包裹法**的优点，直接**在模型训练过程**中**自动选择特征**。它通过学习算法自动选择最重要的特征，使特征选择与模型训练同时进行。
   - **常见方法**：
     - **L1正则化（Lasso回归）**：通过在损失函数中**添加L1正则化项**，使**部分特征的系数变为零，从而进行特征选择。
     - **决策树及其变体（如随机森林、XGBoost）**：树模型的特征重要性得分可以用于选择重要特征。
     - **Elastic Net**：结合L1和L2正则化的优势，在保持模型稀疏性的同时，减少了多重共线性的影响，进行特征选择和模型优化。
   - **优点**：特征选择与模型训练同时完成，考虑特征间的相互作用，效率较高。
   - **缺点**：需要根据特定算法来进行选择，不具有模型无关性。

### 4. 其他方法
   - **PCA（主成分分析）**：虽然PCA是降维方法，但在某些场景下可以间接用于特征选择。通过对数据进行线性变换，将多个原始特征组合成少数几个主成分。
   - **LDA（线性判别分析）**：常用于分类问题的降维，也可以视作一种特征选择方法。
   - **基于稳定性选择（Stability Selection）**：通过在多次子样本集上重复训练模型，并选择那些在多个子集上都表现重要的特征，从而增强选择的鲁棒性。

### 5. 选择方法的应用场景
   - **过滤法**适用于快速预筛选大量特征的情况，计算效率高，但可能丢失特征之间的组合信息。
   - **包裹法**在特征数不多时（例如几十个或上百个）效果较好，能找到最佳的特征组合，但计算开销较大。
   - **嵌入法**通常适用于大多数场景，尤其是使用线性模型（Lasso）或树模型时，既能训练模型又能自动选择特征。

### 总结
下面是特征选择方法的总结表格，保留了原有的描述信息：

| **方法类别**                  | **原理**                                                                                     | **常见方法**                                                                                                                                           | **优点**                                                   | **缺点**                                                     | **适用场景**                                                    |
|---------------------------|--------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------|------------------------------------------------------------|------------------------------------------------------------|
| **过滤法（Filter Methods）**    | 独立于模型，基于统计指标对特征评分，并选择得分较高的特征。                                                                    | - 方差选择法：剔除方差较小的特征<br>- 皮尔森相关系数：选择线性相关性高的特征<br>- 互信息：选择信息增益大的特征                                           | 计算效率高，易于实现                                           | 未考虑特征间相互作用，可能遗漏重要的组合特征                                  | 快速预筛选大量特征的情况，适合初步筛选特征                                   |
| **包裹法（Wrapper Methods）**    | 通过训练模型评估特征子集表现，使用搜索策略找到最优特征组合。                                                                    | - 递归特征消除（RFE）：删除不重要的特征<br>- 前向选择：逐步添加性能提升最大的特征<br>- 后向消除：逐步移除对模型性能影响小的特征                           | 能考虑特征间的相互作用，适合复杂任务                                      | 计算开销大，训练多个模型耗时长                                             | 特征数较少（几十到上百个），适合需要精确特征选择的任务                               |
| **嵌入法（Embedded Methods）**   | 结合过滤法和包裹法的优点，在模型训练过程中选择特征。                                                                        | - L1正则化（Lasso回归）：通过L1正则化项使部分特征系数为零<br>- 决策树及其变体（随机森林、XGBoost）：根据特征重要性评分选择特征<br>- Elastic Net：结合L1和L2正则化 | 特征选择与模型训练同时进行，考虑特征间相互作用，效率高                              | 需要根据特定算法选择，不具有模型无关性                                        | 适合使用线性模型（如Lasso）或树模型的场景，大多数现代复杂模型都适用                     |
| **其他方法**                  | PCA、LDA等方法虽然是降维方法，但可间接用于特征选择。                                                                          | - PCA：通过线性变换将多个特征组合成少数几个主成分<br>- LDA：常用于分类问题的降维方法<br>- 稳定性选择（Stability Selection）：通过在子样本集上选择表现稳定的特征 | 能够进行有效降维，有时可以间接用于特征选择                                   | 降维后特征解释性较弱                                               | 数据维度较高的情况下，可以用作降维手段，间接提高特征选择效果                             |

- **过滤法**：速度快，适合**预处理**大量特征，但可能丢失特征间的组合信息。
- **包裹法**：精度高，适合特征数较少且精度要求高的任务，但计算成本大。
- **嵌入法**：**性能和效率兼顾，适合大多数场景，尤其是使用线性模型（Lasso）或树模型时**。
- **其他方法**：如PCA、LDA等可以作为降维手段，间接用于特征选择，适合高维数据的场景。

选择合适的特征选择方法能够有效提升模型性能，降低训练时间，避免过拟合。

## 3. 特征构造（Feature Construction）

**特征构造**是通过**对已有特征进行组合、变换或生成新特征**来增强模型表达能力的过程。它可以将隐含的关系显式化，提高模型的拟合能力。

| 类别                | 主要方法                                             | 适用场景                                                   |
|---------------------|------------------------------------------------------|------------------------------------------------------------|
| 数值特征构造        | 变换、分箱                                           | 处理数值特征、非线性关系                                   |
| 类别特征构造        | 编码、组合                                           | 处理类别特征、捕捉特征间关系                               |
| 时间特征构造        | 时间提取、周期特征、时间差                           | 时间序列数据、周期性特征                                   |
| 文本特征构造        | 词袋、TF-IDF、词向量、N-grams                        | 文本数据、自然语言处理                                     |
| 特征交互与组合      | 特征交互、多项式特征                                 | 捕捉特征间的复杂关系，适合增强线性模型的非线性拟合能力     |
| 聚合与统计特征      | 聚合、统计、窗口聚合                                 | 大规模表格数据、时间序列数据                               |
| 生成模型特征        | 降维、聚类、自编码器生成特征                         | 复杂高维数据、需要特征压缩的场景                           |
| 特征选择与构造结合  | 筛选后构造、嵌入法生成特征                           | 大规模数据集、特征选择与构造结合的场景                     |

特征构造是一项创造性和技术性并重的任务，需要结合领域知识、数据分析技巧以及机器学习经验来挖掘出更有利于模型训练的特征，从而提升模型的表现。
## 4. 特征缩放

 1. **归一化**：通常是指将数据**缩放到一个特定的范围，如[0, 1]**。目的是**让不同特征的值处于相同的尺度上**，【同时也有消除不同特征量纲的影响的作用】**大范围的特征值可能会导致梯度更新过慢或不稳定**。
 2. **标准化**：是指对数据进行**均值0、标准差1**的变换，**更关注数据的分布形态**。目的是**消除不同特征的物理单位和量纲（如重量、温度、距离等）差异**，同时保持特征间的相对比例关系。

### 4.1 归一化（Normalization）
**归一化**将**特征值缩放到 [0, 1] 之间**，常用于以下算法中：
- **K 近邻算法（KNN）**：归一化后减少**不同特征尺度对距离计算的影响**。能够避免特征量纲不同带来的距离计算问题。【与数据的分布无关】
- **神经网络**：将输入特征值缩放至 [0, 1]，有助于加快模型收敛。
- **聚类算法（如 K-Means）**：归一化避免特征尺度不同造成聚类结果偏差。

**Min - Max归一化**：将特征缩放到指定范围（通常为[0, 1]），公式为：
$$ x' = \frac{x - x_{\min}}{x_{\max} - x_{\min}} $$
### 4.2 标准化（Standardization）
**标准化**将**特征值转化为均值为 0、方差为 1 的标准正态分布**，常用于以下算法中：
- **线性回归**：标准化能够提升参数解释性，并避免部分特征影响过大。
- **逻辑回归**：标准化能够使梯度下降更快地收敛。
- **支持向量机（SVM）**：标准化后距离计算更稳定。
- **主成分分析（PCA）**：标准化防止某些方差大的特征主导主成分的计算。

**Z - score标准化：** 将数据转换为均值为0，方差为1的标准正态分布，公式为：
$$ x' = \frac{x - \mu}{\sigma} $$
### BN、LN、IN、GN
以下是归一化方法对比总结，其中加入了每种归一化方法的原理：

| 归一化方法         | 原理                                                         | 适用场景                  | 优点                            | 缺点                              |
|--------------------|--------------------------------------------------------------|---------------------------|---------------------------------|-----------------------------------|
| **批归一化（BN）**  | 对**一个批量**中的所有样本的**同一通道**进行归一化，基于批次的均值和方差调整  | 卷积网络、全连接网络       | 加快收敛，正则化，适应大批量训练 | 对小批次敏感，序列任务效果差      |
| **层归一化（LN）**  | 对**单个样本的所有通道**进行归一化，不依赖批量，计算层内均值和方差  | RNN、Transformer、序列任务 | 适应小批次训练，不依赖批次大小   | 计算量较大，收敛可能稍慢          |
| **实例归一化（IN）**| 对**单张图像的每个通道**分别独立进行归一化，计算每个样本的通道内均值和方差 | 图像生成、风格迁移         | 对风格敏感，适用于生成任务        | 不适合分类任务，无法捕捉全局信息   |
| **组归一化（GN）**  | 将**单个样本的特征通道分组**，对每一组进行归一化，计算组内均值和方差       | 小批次训练，卷积网络       | 适合小批次，不依赖批次大小       | 对卷积核大小和通道数较敏感         |
| **权重归一化（WN）**| 对神经元的**权重**向量**进行归一化**，将**方向和长度分开重新参数化**     | 卷积网络、全连接网络、生成模型 | 加速收敛，提高稳定性             | 效果不一定显著，某些任务中不如BN  |

> 注意，虽然他们是叫做归一化（批归一化、层归一化、实例归一化），是将多个输入特征**归一化为均值为 0、方差为 1 的分布**，使得网络的各层输入保持在较为稳定的范围内。**本质上是进行标准化。再进行引入两个可学习参数 γ 和 𝛽，分别表示缩放和平移操作。**
> 
> BN、LN、IN、GN 等归一化方法都**包含了标准化**的步骤，即它们都会将激活值调整为均值为 0、方差为 1 的分布，关键区别在于这些方法在不同的范围内计算均值和方差，以适应不同的训练场景和模型结构：

**注意：** 虽然它们方法名字中带“归一化”（批归一化、层归一化、实例归一化、组归一化），但它们的核心操作本质上是标准化，将多个输入特征**归一化为均值为 0、方差为 1 的分布**，使得网络的各层输入保持在较为稳定的范围内。本质上是进行标准化。再进行引入两个可学习参数 γ 和 𝛽，分别表示缩放和平移操作。

# 正则化
### L1 正则化（Lasso）
#### 原理

L1正则化通过在损失函数中加入**权重的绝对值和**来约束模型复杂度。其目标函数为：
$$ \min \left( \frac{1}{2m} \sum_{i = 1}^m (y_i - \hat{y}_i)^2 + \lambda \sum_{j = 1}^n |w_j| \right) $$
- 其中，λ是正则化强度，\( w_j \)是第j个特征的权重。
#### 使用场景
- **特征选择**：L1 正则化能够将部分不重要的特征权重缩减为 0，从而实现**特征选择**。
- **高维稀疏数据集**：如基因数据分析，模型能够自动去除无关特征。

#### 优缺点
- **优点**：生成稀疏解，易于解释，自动选择重要的特征。
- **缺点**：对特征高度相关的数据，随机选择特征，模型不稳定。


### L2 正则化（Ridge）

#### 原理

L2正则化通过在损失函数中加入权重的平方和来约束模型复杂度。其目标函数为：
$$ \min \left( \frac{1}{2m} \sum_{i = 1}^m (y_i - \hat{y}_i)^2 + \lambda \sum_{j = 1}^n w_j^2 \right) $$
- 其中，λ是正则化强度，$w_j$是第j个特征的权重。
#### 使用场景
- **多重共线性问题**：在特征间存在**多重共线性的情况下**，L2 正则化能够**减小模型方差，防止模型对数据的过拟合**。
- **回归任务**：如岭回归（Ridge Regression）中常用来提升**模型鲁棒性**。

#### 优缺点
- **优点**：防止模型过拟合，能有效处理特征多重共线性问题。
- **缺点**：不能进行特征选择，所有特征权重都被减小。


### Elastic Net 正则化
#### 定义
Elastic Net 是 L1 和 L2 正则化的结合，它**同时引入了 L1 和 L2 正则化项**，在获得稀疏解的同时，保持一定的平滑性。

#### 公式

$$ J_{\text{ElasticNet}}(\theta) = J(\theta) + \lambda_1 \sum_i |\theta_i| + \lambda_2 \sum_i \theta_i^2 $$
其中，$\lambda_1$和$\lambda_2$控制L1和L2正则化的权重。
#### 优点
- 结合了 L1 和 L2 正则化的优点，既能够稀疏化模型，又不会完全忽略相关性特征。
- 对高维数据和特征之间存在高度相关性的数据表现良好。
#### 缺点
- 相比于单独使用 L1 或 L2 正则化，它有更多的超参数需要调节。
#### 应用场景
- 常用于具有高维特征的数据集，特别是在需要稀疏化的同时，又不希望完全丢失特征之间相关性的信息。

### Dropout
#### 原理
Dropout 是一种用于深度神经网络的正则化方法。训练过程中，Dropout **随机将部分神经元的输出设置为 0**，**防止神经元对特定特征的依赖**，从而提升模型的泛化能力。类似集成学习，每次生成的都不一样。丢弃概率 \(p\)，通常设置为 **0.2 到 0.5**。

#### 使用场景
- **深度神经网络**：在深度学习中广泛应用，如卷积神经网络（CNN）、循环神经网络（RNN）等。
- **避免过拟合**：尤其在模型复杂、训练数据较少的场景中，能够有效**降低过拟合风险**。

#### 优缺点
- **优点**：有效防止过拟合，提升模型鲁棒性。
- **缺点**：训练时间较长，推理过程中不适用。

### 早停法（Early Stopping）
#### 原理
早停法是一种**防止模型过拟合的策略**。在训练过程中，**监控验证集的误差变化，当验证集误差不再降低时，提前停止训练，防止模型过拟合到训练数据**。

#### 使用场景
- **深度学习**：几乎适用于所有深度学习模型，在神经网络训练中常用，防止训练过度拟合。
- **梯度下降优化**：在任何基于梯度下降的优化过程中均可使用，如线性回归、逻辑回归等。

#### 优缺点
- **优点**：简单有效，能够动态调节训练过程。
- **缺点**：需要合理设置停止条件，可能导致模型欠拟合。

### Batch Normalization (BN)

虽然 Batch Normalization（BN）通常被认为是一种加速训练的技巧，但它也有正则化的效果。BN 通过对每一批次的输入进行归一化，使得模型训练更加稳定，防止过拟合。

#### 原理
BN 通过将每个批次的激活值标准化为均值为 0，方差为 1，然后通过可学习的缩放和平移参数恢复特征分布。由于**批次间的变化引入了一定的噪声**，这对模型有一定的正则化作用。
#### 使用场景
- 广泛应用于卷积神经网络（CNN）和全连接网络（FCN）中。
#### 优点
- 提升训练速度，并有一定的正则化效果。
- 适合卷积神经网络和全连接神经网络，能有效减少过拟合。

#### 缺点
- 在小批量训练时效果不稳定。
- 引入了额外的计算开销。

### 权重衰减（Weight Decay）

权重衰减是一种通过**直接对权重进行衰减**的正则化方法，它**等价于 L2 正则化**。

#### 原理
在每次**权重更新时，加入一个权重衰减项**，使得权重参数逐渐减小，从而**防止权重变得过大，减少模型的复杂度**。
权重衰减直接在**梯度更新中对权重施加一个额外的缩减项**，而**不需要在损失函数中添加正则化项**。也就是说，权重衰减是通过直接操作梯度更新公式中的权重来实现的。

**公式：**
$$
\theta = \theta - \alpha \cdot \frac{\partial L_{\text{data}}}{\partial \theta} - \alpha \lambda \theta 
$$

- 其中：
	- α是学习率。
	- λ是权重衰减系数。
	- θ是模型的权重。

λ 是正则化系数，控制惩罚项的强度。该惩罚项会在每次梯度更新时对权重施加一个减小的力度，从而限制权重的增长。
> **L2正则化和权重衰减**目标一致、数学形式相似，但是并不是同一种手段：
> 1. **实现方式**：
>    - **L2 正则化**：在传统的 L2 正则化中，**惩罚项是直接添加在损失函数中**。因此，反向传播时会计算这个惩罚项的梯度，并将它加入到权重的更新中。优化器仅对 `Loss`求导。
>    - **权重衰减**：在权重衰减中，**惩罚项不直接添加到损失函数中，而是在梯度更新时作为一个附加的“权重缩小”操作**。在每次更新时，优化器会自动将权重按比例缩小。例如，对于SGD 优化器，权重更新公式变成：
> 
> 
> 
> $$w = w - \alpha \cdot \frac{\partial L_{\text{loss}}}{\partial w} - \alpha \cdot \lambda \cdot w$$
> 
> 这里，$\alpha \cdot \lambda \cdot w$是直接对权重施加的缩小因子，而不影响梯度方向。
> 
> 
> 2. **优化器依赖**：
>    - **L2 正则化**：**不依赖于特定的优化器**。正则项直接通过损失函数梯度传播，适用于所有优化器。
>    - **权重衰减**：有些优化器（如 AdamW）在实现时将权重衰减项独立处理，而不会将其纳入损失的反向传播中。

#### 使用场景
- 与 SGD 等优化器配合使用效果较好，尤其适用于大型神经网络，可以防止权重过大导致的过拟合。对于 Adam 优化器，建议使用 AdamW 版本来获得更合适的权重衰减效果。

#### 优点
- 类似于 L2 正则化，简单易用，有效减少过拟合。
#### 缺点
- 与 L2 正则化非常相似，但在某些优化器（如 Adam）中，权重衰减的实现可能会与 L2 正则化略有不同。在这些情况下，直接使用 L2 正则化可能会更符合预期的效果。

### 剪枝（Pruning）
剪枝通常在模型训练完成后进行，作为一种后处理技术。例如**决策树**中的剪枝操作。

#### 原理

剪枝通过**删除神经网络中重要性较低的连接或神经元**，减少模型规模，从而达到简化网络的目的。剪枝不仅可以**减少计算量和存储需求**，还能在一定程度上**防止过拟合**，使模型在推理时更加高效。

#### 应用场景
- **移动和嵌入式设备**：剪枝特别适用于资源受限的设备（如手机、嵌入式系统、物联网设备）上，以减小模型尺寸和降低推理时间。
- **深度学习模型加速**：剪枝广泛用于加速深度神经网络的推理过程，特别是在需要实时处理的任务中，如自动驾驶、图像识别等。
- **大规模模型压缩**：在大规模模型（如大规模卷积神经网络、语言模型）中，剪枝可以显著减少计算量，使得模型更高效地运行。

#### 优点
- **减少模型复杂度**：剪枝可以显著减少网络中的参数，降低计算和内存需求，使得模型更适合在资源有限的设备上（如移动设备、嵌入式系统）运行。
- **提高模型的泛化能力**：通过移除不重要的权重和神经元，减少模型对特定数据特征的过拟合，从而提高泛化能力。
- **加速推理**：剪枝后的模型由于参数减少，推理速度得到显著提升。

#### 缺点
- **需要额外的剪枝步骤**
- **可能影响模型性能**：如果剪枝不当，可能会削弱模型的表现，模型的准确性可能会大幅下降。
- **需要重新训练**：剪枝后的模型有时需要重新微调或训练，以恢复模型性能。

以下是关于常见正则化方法的总结表格：

| **正则化方法**          | **原理**                                                                                       | **使用场景**                                                     | **优点**                                                         | **缺点**                                                         |
|-------------------------|------------------------------------------------------------------------------------------------|------------------------------------------------------------------|------------------------------------------------------------------|------------------------------------------------------------------|
| **L1 正则化** (Lasso)   | 通过增加权重绝对值惩罚项，实现特征稀疏化，部分权重缩减为 0。                                      | 高维稀疏数据集，特征选择任务。                                     | 生成稀疏解，易于解释，自动选择重要的特征。                       | 对特征高度相关的数据，可能随机选择特征，导致模型不稳定。           |
| **L2 正则化** (Ridge)   | 通过增加权重平方和惩罚项，减小权重大小，防止权重过大。                                            | 多重共线性问题、回归任务，如岭回归。                                | 防止模型过拟合，处理特征多重共线性问题，模型更加鲁棒。             | 无法进行特征选择，所有特征权重都被减小。                          |
| **Elastic Net 正则化**  | L1 和 L2 正则化结合，既稀疏化模型，又保留相关性特征。                                              | 高维特征的数据集，稀疏化和相关性特征共存的场景。                    | 结合 L1 和 L2 优点，稀疏化与平滑化并存，适用于高维数据。            | 增加了超参数调节的复杂性。                                         |
| **Dropout**             | 训练时随机丢弃部分神经元输出，防止神经元对特定特征的依赖，提升泛化能力。                            | 深度神经网络，CNN、RNN，适合复杂模型或数据较少的场景。               | 有效防止过拟合，提升模型鲁棒性。                                  | 训练时间较长，推理时不适用。                                       |
| **早停法** (Early Stopping) | 监控验证集误差，验证集误差不再下降时提前停止训练，防止过拟合。                                   | 深度学习模型，梯度下降优化任务，如线性回归、逻辑回归。               | 简单有效，动态调节训练过程，减少过拟合。                           | 需要合理设置停止条件，可能导致欠拟合。                              |
| **Batch Normalization (BN)** | 对每一批次的输入进行归一化，保持训练过程中的稳定性，并有一定正则化效果。                         | 卷积神经网络和全连接神经网络，适用于大批量训练。                      | 加速训练，减少过拟合，提升模型稳定性。                             | 小批量训练时效果不稳定，增加计算开销。                              |
| **权重衰减** (Weight Decay) | 在每次权重更新时加入权重衰减项，防止权重过大，等价于 L2 正则化。                                 | 大规模神经网络，常与 SGD、AdamW 等优化器配合使用。                   | 简单有效，减少过拟合，类似 L2 正则化。                             | 与 L2 略有不同，某些优化器中的效果不同。                            |
| **剪枝** (Pruning)       | 训练后移除神经网络中不重要的连接或神经元，减少模型规模，降低计算量，提升泛化能力。                | 移动设备、嵌入式系统、大规模模型压缩，适合资源受限设备和加速任务。     | 减少模型复杂度，提升推理速度，适合资源受限设备。                   | 需要额外剪枝步骤，可能影响模型性能，需要重新训练。                   |

这个表格总结了常见的正则化方法，涵盖了其工作原理、使用场景、优点和缺点。根据具体任务和数据集，可以选择合适的正则化方法来提高模型的泛化能力和训练效率。

---
# 强化学习（Reinforcement Learning）
### 1. Q 学习（Q-Learning）
**Q 学习**是一种**基于值函数的强化学习算法**，用于**在离散状态和动作空间中学习最优策略**。它通过**更新 Q 值表来估计状态-动作对的价值，从而指导智能体在环境中的行为**。
#### Q 学习原理


Q学习通过Bellman方程更新Q值：

$$Q(s, a) = Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)] $$

其中：
- $Q(s, a)$为当前状态s执行动作a的价值估计。
- α为学习率，控制学习的速度。
- γ为折扣因子，控制未来奖励的影响。
- r 为当前奖励，$s'$为执行动作后到达的下一状态。
#### 应用场景
- **离散状态和动作空间的决策问题**：如迷宫导航、棋类游戏、路径规划等。
- **资源管理**：在云计算、通信网络中分配资源。
#### 优缺点
- **优点**：无需环境模型，能够在部分可观

测环境中学习最优策略。
- **缺点**：高维状态空间下需要大量存储，训练时间长。
### 2. 深度 Q 网络（DQN, Deep Q Network）
**DQN**是结合深度学习和 Q 学习的一种算法，使用神经网络逼近 Q 值函数，能够在高维状态空间中学习有效策略。
#### DQN 原理
- **Q 值逼近**：用神经网络代替传统 Q 学习中的 Q 表，网络输入状态 \( s \)，输出各个动作的 Q 值。
- **经验回放（Experience Replay）**：将智能体的经验存储到回放池中，训练时从中随机抽取小批量经验，打破样本间相关性，提升训练稳定性。
- **目标网络（Target Network）**：引入目标网络来计算 Q 值更新中的目标值，减少训练过程中的不稳定性。

#### 应用场景
- **高维状态空间的决策问题**：如 Atari 游戏、无人驾驶、自动驾驶等。
- **复杂策略学习**：如复杂的游戏 AI、智能交通控制等。
#### 优缺点
- **优点**：能够处理高维状态空间和复杂策略问题，拓展了 Q 学习的应用范围。
- **缺点**：训练不稳定，对超参数敏感，训练时间长。


# 历史文章回顾
[机器学习笔记——损失函数、代价函数和KL散度](https://blog.csdn.net/haopinglianlian/article/details/143831958?spm=1001.2014.3001.5502)
