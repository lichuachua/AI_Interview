<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>机器学习笔记——特征工程、正则化、强化学习</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__html"><blockquote>
<p>大家好，这里是好评笔记，公主号：Goodnote，专栏文章私信限时Free。本笔记介绍机器学习中常见的特征工程方法、正则化方法和简要介绍强化学习。</p>
</blockquote>
<p><img src="https://i-blog.csdnimg.cn/direct/16b570199b7b4484a8f44a8a044ed1b0.jpeg#pic_center" alt="在这里插入图片描述"></p>
<p></p><div class="toc"><h3>文章目录</h3><ul><li><a href="#Fzeature_Engineering_6">特征工程（Fzeature Engineering）</a></li><ul><li><a href="#1_Feature_Extraction_7">1. 特征提取（Feature Extraction）</a></li><ul><li><a href="#Manual_Feature_Extraction_9">手工特征提取（Manual Feature Extraction）：</a></li><li><a href="#Automated_Feature_Extraction_23">自动特征提取（Automated Feature Extraction）：</a></li></ul><li><a href="#2_Feature_Selection_29">2. 特征选择（Feature Selection）</a></li><ul><li><a href="#1_Filter_Methods_31">1. 过滤法（Filter Methods）</a></li><li><a href="#2_Wrapper_Methods_40">2. 包裹法（Wrapper Methods）</a></li><li><a href="#3_Embedded_Methods_48">3. 嵌入法（Embedded Methods）</a></li><li><a href="#4__57">4. 其他方法</a></li><li><a href="#5__62">5. 选择方法的应用场景</a></li><li><a href="#_67">总结</a></li></ul><li><a href="#3_Feature_Construction_84">3. 特征构造（Feature Construction）</a></li><li><a href="#4__100">4. 特征缩放</a></li><ul><li><a href="#41_Normalization_105">4.1 归一化（Normalization）</a></li><li><a href="#42_Standardization_113">4.2 标准化（Standardization）</a></li><li><a href="#BNLNINGN_122">BN、LN、IN、GN</a></li></ul></ul><li><a href="#_139">正则化</a></li><ul><ul><li><a href="#L1_Lasso_140">L1 正则化（Lasso）</a></li><ul><li><a href="#_141">原理</a></li><li><a href="#_146">使用场景</a></li><li><a href="#_150">优缺点</a></li></ul><li><a href="#L2_Ridge_155">L2 正则化（Ridge）</a></li><ul><li><a href="#_157">原理</a></li><li><a href="#_162">使用场景</a></li><li><a href="#_166">优缺点</a></li></ul><li><a href="#Elastic_Net__171">Elastic Net 正则化</a></li><ul><li><a href="#_172">定义</a></li><li><a href="#_175">公式</a></li><li><a href="#_179">优点</a></li><li><a href="#_182">缺点</a></li><li><a href="#_184">应用场景</a></li></ul><li><a href="#Dropout_187">Dropout</a></li><ul><li><a href="#_188">原理</a></li><li><a href="#_191">使用场景</a></li><li><a href="#_195">优缺点</a></li></ul><li><a href="#Early_Stopping_199">早停法（Early Stopping）</a></li><ul><li><a href="#_200">原理</a></li><li><a href="#_203">使用场景</a></li><li><a href="#_207">优缺点</a></li></ul><li><a href="#Batch_Normalization_BN_211">Batch Normalization (BN)</a></li><ul><li><a href="#_215">原理</a></li><li><a href="#_217">使用场景</a></li><li><a href="#_219">优点</a></li><li><a href="#_223">缺点</a></li></ul><li><a href="#Weight_Decay_227">权重衰减（Weight Decay）</a></li><ul><li><a href="#_231">原理</a></li><li><a href="#_262">使用场景</a></li><li><a href="#_265">优点</a></li><li><a href="#_267">缺点</a></li></ul><li><a href="#Pruning_270">剪枝（Pruning）</a></li><ul><li><a href="#_273">原理</a></li><li><a href="#_277">应用场景</a></li><li><a href="#_282">优点</a></li><li><a href="#_287">缺点</a></li></ul></ul></ul><li><a href="#Reinforcement_Learning_308">强化学习（Reinforcement Learning）</a></li><ul><ul><li><a href="#1_Q_QLearning_309">1. Q 学习（Q-Learning）</a></li><ul><li><a href="#Q__311">Q 学习原理</a></li><li><a href="#_323">应用场景</a></li><li><a href="#_326">优缺点</a></li></ul><li><a href="#2__Q_DQN_Deep_Q_Network_331">2. 深度 Q 网络（DQN, Deep Q Network）</a></li><ul><li><a href="#DQN__333">DQN 原理</a></li><li><a href="#_338">应用场景</a></li><li><a href="#_341">优缺点</a></li></ul></ul></ul><li><a href="#_346">历史文章回顾</a></li></ul></div><p></p>
<h1><a id="Fzeature_Engineering_6"></a>特征工程（Fzeature Engineering）</h1>
<h2><a id="1_Feature_Extraction_7"></a>1. 特征提取（Feature Extraction）</h2>
<p><strong>特征提取</strong>：从<strong>原始数据</strong>中<strong>提取</strong>能够有效表征<strong>数据特征</strong>的过程。它将原始数据转换为适合模型输入的特征表示。</p>
<h3><a id="Manual_Feature_Extraction_9"></a>手工特征提取（Manual Feature Extraction）：</h3>
<ul>
<li><strong>文本数据</strong>：
<ul>
<li><strong>词袋模型</strong>（Bag of Words）：将文本数据转化为<strong>词频向量</strong>，每个单词是一个维度，值为该单词在文本中出现的次数。</li>
<li><strong>TF-IDF</strong>：为词袋模型加入<strong>词频-逆文档频率</strong>（Term Frequency-Inverse Document Frequency），<strong>降低常见词语的权重</strong>，<strong>提升重要词语的权重</strong>。</li>
<li><strong>N-gram</strong>：将连续的 N 个词作为一个特征，捕捉<strong>词语间的局部依赖关系</strong>。</li>
</ul>
</li>
<li><strong>图像数据</strong>：
<ul>
<li><strong>边缘检测</strong>：使用 Sobel 算子、Canny 边缘检测等方法提取图像边缘信息。</li>
<li><strong>SIFT（尺度不变特征变换）</strong>：提取图像的关键点和局部特征，具有尺度不变性和旋转不变性。</li>
<li><strong>HOG（方向梯度直方图）</strong>：将图像分块，并统计每块的梯度方向直方图，用于描述局部形状和纹理特征。</li>
</ul>
</li>
<li><strong>时间序列数据</strong>：
<ul>
<li><strong>移动平均</strong>：对时间序列进行平滑，消除短期波动。</li>
<li><strong>傅里叶变换</strong>：将时间域的信号转化为频域信号，分析数据的周期性。</li>
<li><strong>窗口函数</strong>：将时间序列分为若干窗口，分别计算每个窗口的统计特征，如均值、方差等。</li>
</ul>
</li>
</ul>
<h3><a id="Automated_Feature_Extraction_23"></a>自动特征提取（Automated Feature Extraction）：</h3>
<ul>
<li>使用<strong>卷积神经网络（CNN）</strong>：从图像中自动提取高级特征，如边缘、纹理、形状等。</li>
<li>使用<strong>循环神经网络（RNN）</strong>：处理时间序列数据，捕捉长时间依赖关系。</li>
<li>使用<strong>BERT（Transformer）</strong>：通过自监督学习自动提取上下文敏感的文本特征。</li>
<li><strong>自动编码器（Autoencoder）</strong>：使用无监督学习从数据中提取低维特征表示，捕捉数据的潜在结构和模式。</li>
</ul>
<h2><a id="2_Feature_Selection_29"></a>2. 特征选择（Feature Selection）</h2>
<p>特征选择（Feature Selection）是指<strong>从原始特征集中挑选出与目标任务最相关的特征</strong>，以提高模型的性能、减少训练时间以及降低过拟合的风险。特征选择方法主要分为三类：<strong>过滤法（Filter Methods）</strong>、<strong>包裹法（Wrapper Methods）</strong> 和 <strong>嵌入法（Embedded Methods）</strong>。</p>
<h3><a id="1_Filter_Methods_31"></a>1. 过滤法（Filter Methods）</h3>
<ul>
<li><strong>原理</strong>：独立于模型，<strong>训练前</strong>首先根据某些<strong>统计指标对特征进行评分</strong>，然后选择得分较高的特征。这种方法不依赖于特定的学习算法，只是<strong>基于数据本身的特性进行筛选</strong>。</li>
<li><strong>常见方法</strong>：
<ul>
<li><strong>方差选择法</strong>：<strong>剔除方差较小的特征</strong>，认为方差小的特征对目标值影响小。</li>
<li><strong>皮尔森相关系数</strong>：计算特征与目标变量之间的线性相关性，<strong>选择线性相关性较高的特征</strong>。</li>
<li><strong>互信息</strong>：衡量特征与目标变量之间的<strong>信息增益</strong>，选择信息量大的特征。</li>
</ul>
</li>
<li><strong>优点</strong>：<strong>计算效率高，易于实现</strong>。</li>
<li><strong>缺点</strong>：未考虑特征之间的相互作用，可能遗漏组合特征的重要性。</li>
</ul>
<h3><a id="2_Wrapper_Methods_40"></a>2. 包裹法（Wrapper Methods）</h3>
<ul>
<li><strong>原理</strong>：在<strong>训练中</strong>，通过训练模型<strong>评估特征子集的表现</strong>，使用搜索策略找到对目标任务最优的<strong>特征组合</strong>。包裹法直接<strong>根据模型的性能进行选择</strong>，通常<strong>通过交叉验证</strong>来评估特征子集的好坏。</li>
<li><strong>常见方法</strong>：
<ul>
<li><strong>前向选择（Forward Selection）</strong>：从<strong>空集开始</strong>，<strong>逐步添加</strong>对模型性能提升最大的特征。</li>
<li><strong>后向消除（Backward Elimination）</strong>：从<strong>所有特征开始</strong>，<strong>逐步移除</strong>对模型性能影响最小的特征。</li>
</ul>
</li>
<li><strong>优点</strong>：能够<strong>考虑特征之间的相互作用</strong>，适合复杂的特征选择任务。</li>
<li><strong>缺点</strong>：计算开销大，尤其是当特征数目较多时，训练多个模型的过程会非常耗时。</li>
</ul>
<h3><a id="3_Embedded_Methods_48"></a>3. 嵌入法（Embedded Methods）</h3>
<ul>
<li><strong>原理</strong>：嵌入法<strong>结合了过滤法和包裹法</strong>的优点，直接<strong>在模型训练过程</strong>中<strong>自动选择特征</strong>。它通过学习算法自动选择最重要的特征，使特征选择与模型训练同时进行。</li>
<li><strong>常见方法</strong>：
<ul>
<li><strong>L1正则化（Lasso回归）</strong>：通过在损失函数中<strong>添加L1正则化项</strong>，使**部分特征的系数变为零，从而进行特征选择。</li>
<li><strong>决策树及其变体（如随机森林、XGBoost）</strong>：树模型的特征重要性得分可以用于选择重要特征。</li>
<li><strong>Elastic Net</strong>：结合L1和L2正则化的优势，在保持模型稀疏性的同时，减少了多重共线性的影响，进行特征选择和模型优化。</li>
</ul>
</li>
<li><strong>优点</strong>：特征选择与模型训练同时完成，考虑特征间的相互作用，效率较高。</li>
<li><strong>缺点</strong>：需要根据特定算法来进行选择，不具有模型无关性。</li>
</ul>
<h3><a id="4__57"></a>4. 其他方法</h3>
<ul>
<li><strong>PCA（主成分分析）</strong>：虽然PCA是降维方法，但在某些场景下可以间接用于特征选择。通过对数据进行线性变换，将多个原始特征组合成少数几个主成分。</li>
<li><strong>LDA（线性判别分析）</strong>：常用于分类问题的降维，也可以视作一种特征选择方法。</li>
<li><strong>基于稳定性选择（Stability Selection）</strong>：通过在多次子样本集上重复训练模型，并选择那些在多个子集上都表现重要的特征，从而增强选择的鲁棒性。</li>
</ul>
<h3><a id="5__62"></a>5. 选择方法的应用场景</h3>
<ul>
<li><strong>过滤法</strong>适用于快速预筛选大量特征的情况，计算效率高，但可能丢失特征之间的组合信息。</li>
<li><strong>包裹法</strong>在特征数不多时（例如几十个或上百个）效果较好，能找到最佳的特征组合，但计算开销较大。</li>
<li><strong>嵌入法</strong>通常适用于大多数场景，尤其是使用线性模型（Lasso）或树模型时，既能训练模型又能自动选择特征。</li>
</ul>
<h3><a id="_67"></a>总结</h3>
<p>下面是特征选择方法的总结表格，保留了原有的描述信息：</p>

<table>
<thead>
<tr>
<th><strong>方法类别</strong></th>
<th><strong>原理</strong></th>
<th><strong>常见方法</strong></th>
<th><strong>优点</strong></th>
<th><strong>缺点</strong></th>
<th><strong>适用场景</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>过滤法（Filter Methods）</strong></td>
<td>独立于模型，基于统计指标对特征评分，并选择得分较高的特征。</td>
<td>- 方差选择法：剔除方差较小的特征<br>- 皮尔森相关系数：选择线性相关性高的特征<br>- 互信息：选择信息增益大的特征</td>
<td>计算效率高，易于实现</td>
<td>未考虑特征间相互作用，可能遗漏重要的组合特征</td>
<td>快速预筛选大量特征的情况，适合初步筛选特征</td>
</tr>
<tr>
<td><strong>包裹法（Wrapper Methods）</strong></td>
<td>通过训练模型评估特征子集表现，使用搜索策略找到最优特征组合。</td>
<td>- 递归特征消除（RFE）：删除不重要的特征<br>- 前向选择：逐步添加性能提升最大的特征<br>- 后向消除：逐步移除对模型性能影响小的特征</td>
<td>能考虑特征间的相互作用，适合复杂任务</td>
<td>计算开销大，训练多个模型耗时长</td>
<td>特征数较少（几十到上百个），适合需要精确特征选择的任务</td>
</tr>
<tr>
<td><strong>嵌入法（Embedded Methods）</strong></td>
<td>结合过滤法和包裹法的优点，在模型训练过程中选择特征。</td>
<td>- L1正则化（Lasso回归）：通过L1正则化项使部分特征系数为零<br>- 决策树及其变体（随机森林、XGBoost）：根据特征重要性评分选择特征<br>- Elastic Net：结合L1和L2正则化</td>
<td>特征选择与模型训练同时进行，考虑特征间相互作用，效率高</td>
<td>需要根据特定算法选择，不具有模型无关性</td>
<td>适合使用线性模型（如Lasso）或树模型的场景，大多数现代复杂模型都适用</td>
</tr>
<tr>
<td><strong>其他方法</strong></td>
<td>PCA、LDA等方法虽然是降维方法，但可间接用于特征选择。</td>
<td>- PCA：通过线性变换将多个特征组合成少数几个主成分<br>- LDA：常用于分类问题的降维方法<br>- 稳定性选择（Stability Selection）：通过在子样本集上选择表现稳定的特征</td>
<td>能够进行有效降维，有时可以间接用于特征选择</td>
<td>降维后特征解释性较弱</td>
<td>数据维度较高的情况下，可以用作降维手段，间接提高特征选择效果</td>
</tr>
</tbody>
</table><ul>
<li><strong>过滤法</strong>：速度快，适合<strong>预处理</strong>大量特征，但可能丢失特征间的组合信息。</li>
<li><strong>包裹法</strong>：精度高，适合特征数较少且精度要求高的任务，但计算成本大。</li>
<li><strong>嵌入法</strong>：<strong>性能和效率兼顾，适合大多数场景，尤其是使用线性模型（Lasso）或树模型时</strong>。</li>
<li><strong>其他方法</strong>：如PCA、LDA等可以作为降维手段，间接用于特征选择，适合高维数据的场景。</li>
</ul>
<p>选择合适的特征选择方法能够有效提升模型性能，降低训练时间，避免过拟合。</p>
<h2><a id="3_Feature_Construction_84"></a>3. 特征构造（Feature Construction）</h2>
<p><strong>特征构造</strong>是通过<strong>对已有特征进行组合、变换或生成新特征</strong>来增强模型表达能力的过程。它可以将隐含的关系显式化，提高模型的拟合能力。</p>

<table>
<thead>
<tr>
<th>类别</th>
<th>主要方法</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>数值特征构造</td>
<td>变换、分箱</td>
<td>处理数值特征、非线性关系</td>
</tr>
<tr>
<td>类别特征构造</td>
<td>编码、组合</td>
<td>处理类别特征、捕捉特征间关系</td>
</tr>
<tr>
<td>时间特征构造</td>
<td>时间提取、周期特征、时间差</td>
<td>时间序列数据、周期性特征</td>
</tr>
<tr>
<td>文本特征构造</td>
<td>词袋、TF-IDF、词向量、N-grams</td>
<td>文本数据、自然语言处理</td>
</tr>
<tr>
<td>特征交互与组合</td>
<td>特征交互、多项式特征</td>
<td>捕捉特征间的复杂关系，适合增强线性模型的非线性拟合能力</td>
</tr>
<tr>
<td>聚合与统计特征</td>
<td>聚合、统计、窗口聚合</td>
<td>大规模表格数据、时间序列数据</td>
</tr>
<tr>
<td>生成模型特征</td>
<td>降维、聚类、自编码器生成特征</td>
<td>复杂高维数据、需要特征压缩的场景</td>
</tr>
<tr>
<td>特征选择与构造结合</td>
<td>筛选后构造、嵌入法生成特征</td>
<td>大规模数据集、特征选择与构造结合的场景</td>
</tr>
</tbody>
</table><p>特征构造是一项创造性和技术性并重的任务，需要结合领域知识、数据分析技巧以及机器学习经验来挖掘出更有利于模型训练的特征，从而提升模型的表现。</p>
<h2><a id="4__100"></a>4. 特征缩放</h2>
<ol>
<li><strong>归一化</strong>：通常是指将数据<strong>缩放到一个特定的范围，如[0, 1]</strong>。目的是<strong>让不同特征的值处于相同的尺度上</strong>，【同时也有消除不同特征量纲的影响的作用】<strong>大范围的特征值可能会导致梯度更新过慢或不稳定</strong>。</li>
<li><strong>标准化</strong>：是指对数据进行<strong>均值0、标准差1</strong>的变换，<strong>更关注数据的分布形态</strong>。目的是<strong>消除不同特征的物理单位和量纲（如重量、温度、距离等）差异</strong>，同时保持特征间的相对比例关系。</li>
</ol>
<h3><a id="41_Normalization_105"></a>4.1 归一化（Normalization）</h3>
<p><strong>归一化</strong>将<strong>特征值缩放到 [0, 1] 之间</strong>，常用于以下算法中：</p>
<ul>
<li><strong>K 近邻算法（KNN）</strong>：归一化后减少<strong>不同特征尺度对距离计算的影响</strong>。能够避免特征量纲不同带来的距离计算问题。【与数据的分布无关】</li>
<li><strong>神经网络</strong>：将输入特征值缩放至 [0, 1]，有助于加快模型收敛。</li>
<li><strong>聚类算法（如 K-Means）</strong>：归一化避免特征尺度不同造成聚类结果偏差。</li>
</ul>
<p><strong>Min - Max归一化</strong>：将特征缩放到指定范围（通常为[0, 1]），公式为：<br>
<span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><msub><mi>x</mi><mi>min</mi><mo>⁡</mo></msub></mrow><mrow><msub><mi>x</mi><mi>max</mi><mo>⁡</mo></msub><mo>−</mo><msub><mi>x</mi><mi>min</mi><mo>⁡</mo></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex"> x' = \frac{x - x_{\min}}{x_{\max} - x_{\min}} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.8019em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8019em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 2.0963em; vertical-align: -0.836em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.2603em;"><span class="" style="top: -2.314em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mtight">m</span><span class="mtight">a</span><span class="mtight">x</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3175em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mtight">m</span><span class="mtight">i</span><span class="mtight">n</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3175em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mtight">m</span><span class="mtight">i</span><span class="mtight">n</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.836em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span></p>
<h3><a id="42_Standardization_113"></a>4.2 标准化（Standardization）</h3>
<p><strong>标准化</strong>将<strong>特征值转化为均值为 0、方差为 1 的标准正态分布</strong>，常用于以下算法中：</p>
<ul>
<li><strong>线性回归</strong>：标准化能够提升参数解释性，并避免部分特征影响过大。</li>
<li><strong>逻辑回归</strong>：标准化能够使梯度下降更快地收敛。</li>
<li><strong>支持向量机（SVM）</strong>：标准化后距离计算更稳定。</li>
<li><strong>主成分分析（PCA）</strong>：标准化防止某些方差大的特征主导主成分的计算。</li>
</ul>
<p><strong>Z - score标准化：</strong> 将数据转换为均值为0，方差为1的标准正态分布，公式为：<br>
<span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>μ</mi></mrow><mi>σ</mi></mfrac></mrow><annotation encoding="application/x-tex"> x' = \frac{x - \mu}{\sigma} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.8019em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8019em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.9463em; vertical-align: -0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.2603em;"><span class="" style="top: -2.314em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">σ</span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mord mathnormal">μ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.686em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span></p>
<h3><a id="BNLNINGN_122"></a>BN、LN、IN、GN</h3>
<p>以下是归一化方法对比总结，其中加入了每种归一化方法的原理：</p>

<table>
<thead>
<tr>
<th>归一化方法</th>
<th>原理</th>
<th>适用场景</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>批归一化（BN）</strong></td>
<td>对<strong>一个批量</strong>中的所有样本的<strong>同一通道</strong>进行归一化，基于批次的均值和方差调整</td>
<td>卷积网络、全连接网络</td>
<td>加快收敛，正则化，适应大批量训练</td>
<td>对小批次敏感，序列任务效果差</td>
</tr>
<tr>
<td><strong>层归一化（LN）</strong></td>
<td>对<strong>单个样本的所有通道</strong>进行归一化，不依赖批量，计算层内均值和方差</td>
<td>RNN、Transformer、序列任务</td>
<td>适应小批次训练，不依赖批次大小</td>
<td>计算量较大，收敛可能稍慢</td>
</tr>
<tr>
<td><strong>实例归一化（IN）</strong></td>
<td>对<strong>单张图像的每个通道</strong>分别独立进行归一化，计算每个样本的通道内均值和方差</td>
<td>图像生成、风格迁移</td>
<td>对风格敏感，适用于生成任务</td>
<td>不适合分类任务，无法捕捉全局信息</td>
</tr>
<tr>
<td><strong>组归一化（GN）</strong></td>
<td>将<strong>单个样本的特征通道分组</strong>，对每一组进行归一化，计算组内均值和方差</td>
<td>小批次训练，卷积网络</td>
<td>适合小批次，不依赖批次大小</td>
<td>对卷积核大小和通道数较敏感</td>
</tr>
<tr>
<td><strong>权重归一化（WN）</strong></td>
<td>对神经元的<strong>权重</strong>向量<strong>进行归一化</strong>，将<strong>方向和长度分开重新参数化</strong></td>
<td>卷积网络、全连接网络、生成模型</td>
<td>加速收敛，提高稳定性</td>
<td>效果不一定显著，某些任务中不如BN</td>
</tr>
</tbody>
</table><blockquote>
<p>注意，虽然他们是叫做归一化（批归一化、层归一化、实例归一化），是将多个输入特征<strong>归一化为均值为 0、方差为 1 的分布</strong>，使得网络的各层输入保持在较为稳定的范围内。<strong>本质上是进行标准化。再进行引入两个可学习参数 γ 和 𝛽，分别表示缩放和平移操作。</strong></p>
<p>BN、LN、IN、GN 等归一化方法都<strong>包含了标准化</strong>的步骤，即它们都会将激活值调整为均值为 0、方差为 1 的分布，关键区别在于这些方法在不同的范围内计算均值和方差，以适应不同的训练场景和模型结构：</p>
</blockquote>
<p><strong>注意：</strong> 虽然它们方法名字中带“归一化”（批归一化、层归一化、实例归一化、组归一化），但它们的核心操作本质上是标准化，将多个输入特征<strong>归一化为均值为 0、方差为 1 的分布</strong>，使得网络的各层输入保持在较为稳定的范围内。本质上是进行标准化。再进行引入两个可学习参数 γ 和 𝛽，分别表示缩放和平移操作。</p>
<h1><a id="_139"></a>正则化</h1>
<h3><a id="L1_Lasso_140"></a>L1 正则化（Lasso）</h3>
<h4><a id="_141"></a>原理</h4>
<p>L1正则化通过在损失函数中加入<strong>权重的绝对值和</strong>来约束模型复杂度。其目标函数为：<br>
<span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>min</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mi>m</mi></mrow></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>+</mo><mi>λ</mi><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi mathvariant="normal">∣</mi><msub><mi>w</mi><mi>j</mi></msub><mi mathvariant="normal">∣</mi><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex"> \min \left( \frac{1}{2m} \sum_{i = 1}^m (y_i - \hat{y}_i)^2 + \lambda \sum_{j = 1}^n |w_j| \right) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 3.1638em; vertical-align: -1.4138em;"></span><span class="mop">min</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;"><span class="delimsizing size4">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.3214em;"><span class="" style="top: -2.314em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord">2</span><span class="mord mathnormal">m</span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.686em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.6514em;"><span class="" style="top: -1.8723em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.05em;"><span class="pstrut" style="height: 3.05em;"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span><span class="" style="top: -4.3em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.2777em;"><span class=""></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.6944em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span></span><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1944em;"><span class=""></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8641em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mord mathnormal">λ</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.6514em;"><span class="" style="top: -1.8723em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.05em;"><span class="pstrut" style="height: 3.05em;"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span><span class="" style="top: -4.3em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.4138em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0269em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span><span class="mord">∣</span><span class="mclose delimcenter" style="top: 0em;"><span class="delimsizing size4">)</span></span></span></span></span></span></span></span></p>
<ul>
<li>其中，λ是正则化强度，( w_j )是第j个特征的权重。</li>
</ul>
<h4><a id="_146"></a>使用场景</h4>
<ul>
<li><strong>特征选择</strong>：L1 正则化能够将部分不重要的特征权重缩减为 0，从而实现<strong>特征选择</strong>。</li>
<li><strong>高维稀疏数据集</strong>：如基因数据分析，模型能够自动去除无关特征。</li>
</ul>
<h4><a id="_150"></a>优缺点</h4>
<ul>
<li><strong>优点</strong>：生成稀疏解，易于解释，自动选择重要的特征。</li>
<li><strong>缺点</strong>：对特征高度相关的数据，随机选择特征，模型不稳定。</li>
</ul>
<h3><a id="L2_Ridge_155"></a>L2 正则化（Ridge）</h3>
<h4><a id="_157"></a>原理</h4>
<p>L2正则化通过在损失函数中加入权重的平方和来约束模型复杂度。其目标函数为：<br>
<span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>min</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mi>m</mi></mrow></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>+</mo><mi>λ</mi><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mi>w</mi><mi>j</mi><mn>2</mn></msubsup><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex"> \min \left( \frac{1}{2m} \sum_{i = 1}^m (y_i - \hat{y}_i)^2 + \lambda \sum_{j = 1}^n w_j^2 \right) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 3.1638em; vertical-align: -1.4138em;"></span><span class="mop">min</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;"><span class="delimsizing size4">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.3214em;"><span class="" style="top: -2.314em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord">2</span><span class="mord mathnormal">m</span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.686em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.6514em;"><span class="" style="top: -1.8723em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.05em;"><span class="pstrut" style="height: 3.05em;"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span><span class="" style="top: -4.3em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.2777em;"><span class=""></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.6944em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span></span><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1944em;"><span class=""></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8641em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mord mathnormal">λ</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.6514em;"><span class="" style="top: -1.8723em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.05em;"><span class="pstrut" style="height: 3.05em;"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span><span class="" style="top: -4.3em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.4138em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8641em;"><span class="" style="top: -2.453em; margin-left: -0.0269em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em;">j</span></span></span><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.3831em;"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter" style="top: 0em;"><span class="delimsizing size4">)</span></span></span></span></span></span></span></span></p>
<ul>
<li>其中，λ是正则化强度，<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">w_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7167em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0269em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span></span></span></span></span>是第j个特征的权重。</li>
</ul>
<h4><a id="_162"></a>使用场景</h4>
<ul>
<li><strong>多重共线性问题</strong>：在特征间存在<strong>多重共线性的情况下</strong>，L2 正则化能够<strong>减小模型方差，防止模型对数据的过拟合</strong>。</li>
<li><strong>回归任务</strong>：如岭回归（Ridge Regression）中常用来提升<strong>模型鲁棒性</strong>。</li>
</ul>
<h4><a id="_166"></a>优缺点</h4>
<ul>
<li><strong>优点</strong>：防止模型过拟合，能有效处理特征多重共线性问题。</li>
<li><strong>缺点</strong>：不能进行特征选择，所有特征权重都被减小。</li>
</ul>
<h3><a id="Elastic_Net__171"></a>Elastic Net 正则化</h3>
<h4><a id="_172"></a>定义</h4>
<p>Elastic Net 是 L1 和 L2 正则化的结合，它<strong>同时引入了 L1 和 L2 正则化项</strong>，在获得稀疏解的同时，保持一定的平滑性。</p>
<h4><a id="_175"></a>公式</h4>
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>J</mi><mtext>ElasticNet</mtext></msub><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>=</mo><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>+</mo><msub><mi>λ</mi><mn>1</mn></msub><munder><mo>∑</mo><mi>i</mi></munder><mi mathvariant="normal">∣</mi><msub><mi>θ</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><mo>+</mo><msub><mi>λ</mi><mn>2</mn></msub><munder><mo>∑</mo><mi>i</mi></munder><msubsup><mi>θ</mi><mi>i</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex"> J_{\text{ElasticNet}}(\theta) = J(\theta) + \lambda_1 \sum_i |\theta_i| + \lambda_2 \sum_i \theta_i^2 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0962em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: -0.0962em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">ElasticNet</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0962em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 2.3277em; vertical-align: -1.2777em;"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.05em;"><span class="" style="top: -1.8723em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span class="" style="top: -3.05em;"><span class="pstrut" style="height: 3.05em;"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.2777em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0278em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord">∣</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 2.3277em; vertical-align: -1.2777em;"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.05em;"><span class="" style="top: -1.8723em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span class="" style="top: -3.05em;"><span class="pstrut" style="height: 3.05em;"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.2777em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8641em;"><span class="" style="top: -2.453em; margin-left: -0.0278em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.247em;"><span class=""></span></span></span></span></span></span></span></span></span></span></span><br>
其中，<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\lambda_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.8444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\lambda_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.8444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>控制L1和L2正则化的权重。</p>
<h4><a id="_179"></a>优点</h4>
<ul>
<li>结合了 L1 和 L2 正则化的优点，既能够稀疏化模型，又不会完全忽略相关性特征。</li>
<li>对高维数据和特征之间存在高度相关性的数据表现良好。</li>
</ul>
<h4><a id="_182"></a>缺点</h4>
<ul>
<li>相比于单独使用 L1 或 L2 正则化，它有更多的超参数需要调节。</li>
</ul>
<h4><a id="_184"></a>应用场景</h4>
<ul>
<li>常用于具有高维特征的数据集，特别是在需要稀疏化的同时，又不希望完全丢失特征之间相关性的信息。</li>
</ul>
<h3><a id="Dropout_187"></a>Dropout</h3>
<h4><a id="_188"></a>原理</h4>
<p>Dropout 是一种用于深度神经网络的正则化方法。训练过程中，Dropout <strong>随机将部分神经元的输出设置为 0</strong>，<strong>防止神经元对特定特征的依赖</strong>，从而提升模型的泛化能力。类似集成学习，每次生成的都不一样。丢弃概率 (p)，通常设置为 <strong>0.2 到 0.5</strong>。</p>
<h4><a id="_191"></a>使用场景</h4>
<ul>
<li><strong>深度神经网络</strong>：在深度学习中广泛应用，如卷积神经网络（CNN）、循环神经网络（RNN）等。</li>
<li><strong>避免过拟合</strong>：尤其在模型复杂、训练数据较少的场景中，能够有效<strong>降低过拟合风险</strong>。</li>
</ul>
<h4><a id="_195"></a>优缺点</h4>
<ul>
<li><strong>优点</strong>：有效防止过拟合，提升模型鲁棒性。</li>
<li><strong>缺点</strong>：训练时间较长，推理过程中不适用。</li>
</ul>
<h3><a id="Early_Stopping_199"></a>早停法（Early Stopping）</h3>
<h4><a id="_200"></a>原理</h4>
<p>早停法是一种<strong>防止模型过拟合的策略</strong>。在训练过程中，<strong>监控验证集的误差变化，当验证集误差不再降低时，提前停止训练，防止模型过拟合到训练数据</strong>。</p>
<h4><a id="_203"></a>使用场景</h4>
<ul>
<li><strong>深度学习</strong>：几乎适用于所有深度学习模型，在神经网络训练中常用，防止训练过度拟合。</li>
<li><strong>梯度下降优化</strong>：在任何基于梯度下降的优化过程中均可使用，如线性回归、逻辑回归等。</li>
</ul>
<h4><a id="_207"></a>优缺点</h4>
<ul>
<li><strong>优点</strong>：简单有效，能够动态调节训练过程。</li>
<li><strong>缺点</strong>：需要合理设置停止条件，可能导致模型欠拟合。</li>
</ul>
<h3><a id="Batch_Normalization_BN_211"></a>Batch Normalization (BN)</h3>
<p>虽然 Batch Normalization（BN）通常被认为是一种加速训练的技巧，但它也有正则化的效果。BN 通过对每一批次的输入进行归一化，使得模型训练更加稳定，防止过拟合。</p>
<h4><a id="_215"></a>原理</h4>
<p>BN 通过将每个批次的激活值标准化为均值为 0，方差为 1，然后通过可学习的缩放和平移参数恢复特征分布。由于<strong>批次间的变化引入了一定的噪声</strong>，这对模型有一定的正则化作用。</p>
<h4><a id="_217"></a>使用场景</h4>
<ul>
<li>广泛应用于卷积神经网络（CNN）和全连接网络（FCN）中。</li>
</ul>
<h4><a id="_219"></a>优点</h4>
<ul>
<li>提升训练速度，并有一定的正则化效果。</li>
<li>适合卷积神经网络和全连接神经网络，能有效减少过拟合。</li>
</ul>
<h4><a id="_223"></a>缺点</h4>
<ul>
<li>在小批量训练时效果不稳定。</li>
<li>引入了额外的计算开销。</li>
</ul>
<h3><a id="Weight_Decay_227"></a>权重衰减（Weight Decay）</h3>
<p>权重衰减是一种通过<strong>直接对权重进行衰减</strong>的正则化方法，它<strong>等价于 L2 正则化</strong>。</p>
<h4><a id="_231"></a>原理</h4>
<p>在每次<strong>权重更新时，加入一个权重衰减项</strong>，使得权重参数逐渐减小，从而<strong>防止权重变得过大，减少模型的复杂度</strong>。<br>
权重衰减直接在<strong>梯度更新中对权重施加一个额外的缩减项</strong>，而<strong>不需要在损失函数中添加正则化项</strong>。也就是说，权重衰减是通过直接操作梯度更新公式中的权重来实现的。</p>
<p><strong>公式：</strong><br>
<span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>θ</mi><mo>=</mo><mi>θ</mi><mo>−</mo><mi>α</mi><mo>⋅</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>L</mi><mtext>data</mtext></msub></mrow><mrow><mi mathvariant="normal">∂</mi><mi>θ</mi></mrow></mfrac><mo>−</mo><mi>α</mi><mi>λ</mi><mi>θ</mi></mrow><annotation encoding="application/x-tex">
\theta = \theta - \alpha \cdot \frac{\partial L_{\text{data}}}{\partial \theta} - \alpha \lambda \theta 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.7778em; vertical-align: -0.0833em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.4445em;"></span><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 2.0574em; vertical-align: -0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.3714em;"><span class="" style="top: -2.314em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord" style="margin-right: 0.0556em;">∂</span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord" style="margin-right: 0.0556em;">∂</span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">data</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.686em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span><span class="mord mathnormal">λ</span><span class="mord mathnormal" style="margin-right: 0.0278em;">θ</span></span></span></span></span></span></p>
<ul>
<li>其中：
<ul>
<li>α是学习率。</li>
<li>λ是权重衰减系数。</li>
<li>θ是模型的权重。</li>
</ul>
</li>
</ul>
<p>λ 是正则化系数，控制惩罚项的强度。该惩罚项会在每次梯度更新时对权重施加一个减小的力度，从而限制权重的增长。</p>
<blockquote>
<p><strong>L2正则化和权重衰减</strong>目标一致、数学形式相似，但是并不是同一种手段：</p>
<ol>
<li><strong>实现方式</strong>：
<ul>
<li><strong>L2 正则化</strong>：在传统的 L2 正则化中，<strong>惩罚项是直接添加在损失函数中</strong>。因此，反向传播时会计算这个惩罚项的梯度，并将它加入到权重的更新中。优化器仅对 <code>Loss</code>求导。</li>
<li><strong>权重衰减</strong>：在权重衰减中，<strong>惩罚项不直接添加到损失函数中，而是在梯度更新时作为一个附加的“权重缩小”操作</strong>。在每次更新时，优化器会自动将权重按比例缩小。例如，对于SGD 优化器，权重更新公式变成：</li>
</ul>
</li>
</ol>
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>w</mi><mo>=</mo><mi>w</mi><mo>−</mo><mi>α</mi><mo>⋅</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>L</mi><mtext>loss</mtext></msub></mrow><mrow><mi mathvariant="normal">∂</mi><mi>w</mi></mrow></mfrac><mo>−</mo><mi>α</mi><mo>⋅</mo><mi>λ</mi><mo>⋅</mo><mi>w</mi></mrow><annotation encoding="application/x-tex">w = w - \alpha \cdot \frac{\partial L_{\text{loss}}}{\partial w} - \alpha \cdot \lambda \cdot w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6667em; vertical-align: -0.0833em;"></span><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.4445em;"></span><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 2.0574em; vertical-align: -0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.3714em;"><span class="" style="top: -2.314em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord" style="margin-right: 0.0556em;">∂</span><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord" style="margin-right: 0.0556em;">∂</span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">loss</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.686em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.4445em;"></span><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal">λ</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span></span></span></span></span></span></p>
<p>这里，<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>⋅</mo><mi>λ</mi><mo>⋅</mo><mi>w</mi></mrow><annotation encoding="application/x-tex">\alpha \cdot \lambda \cdot w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.4445em;"></span><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal">λ</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span></span></span></span></span>是直接对权重施加的缩小因子，而不影响梯度方向。</p>
<ol start="2">
<li><strong>优化器依赖</strong>：
<ul>
<li><strong>L2 正则化</strong>：<strong>不依赖于特定的优化器</strong>。正则项直接通过损失函数梯度传播，适用于所有优化器。</li>
<li><strong>权重衰减</strong>：有些优化器（如 AdamW）在实现时将权重衰减项独立处理，而不会将其纳入损失的反向传播中。</li>
</ul>
</li>
</ol>
</blockquote>
<h4><a id="_262"></a>使用场景</h4>
<ul>
<li>与 SGD 等优化器配合使用效果较好，尤其适用于大型神经网络，可以防止权重过大导致的过拟合。对于 Adam 优化器，建议使用 AdamW 版本来获得更合适的权重衰减效果。</li>
</ul>
<h4><a id="_265"></a>优点</h4>
<ul>
<li>类似于 L2 正则化，简单易用，有效减少过拟合。</li>
</ul>
<h4><a id="_267"></a>缺点</h4>
<ul>
<li>与 L2 正则化非常相似，但在某些优化器（如 Adam）中，权重衰减的实现可能会与 L2 正则化略有不同。在这些情况下，直接使用 L2 正则化可能会更符合预期的效果。</li>
</ul>
<h3><a id="Pruning_270"></a>剪枝（Pruning）</h3>
<p>剪枝通常在模型训练完成后进行，作为一种后处理技术。例如<strong>决策树</strong>中的剪枝操作。</p>
<h4><a id="_273"></a>原理</h4>
<p>剪枝通过<strong>删除神经网络中重要性较低的连接或神经元</strong>，减少模型规模，从而达到简化网络的目的。剪枝不仅可以<strong>减少计算量和存储需求</strong>，还能在一定程度上<strong>防止过拟合</strong>，使模型在推理时更加高效。</p>
<h4><a id="_277"></a>应用场景</h4>
<ul>
<li><strong>移动和嵌入式设备</strong>：剪枝特别适用于资源受限的设备（如手机、嵌入式系统、物联网设备）上，以减小模型尺寸和降低推理时间。</li>
<li><strong>深度学习模型加速</strong>：剪枝广泛用于加速深度神经网络的推理过程，特别是在需要实时处理的任务中，如自动驾驶、图像识别等。</li>
<li><strong>大规模模型压缩</strong>：在大规模模型（如大规模卷积神经网络、语言模型）中，剪枝可以显著减少计算量，使得模型更高效地运行。</li>
</ul>
<h4><a id="_282"></a>优点</h4>
<ul>
<li><strong>减少模型复杂度</strong>：剪枝可以显著减少网络中的参数，降低计算和内存需求，使得模型更适合在资源有限的设备上（如移动设备、嵌入式系统）运行。</li>
<li><strong>提高模型的泛化能力</strong>：通过移除不重要的权重和神经元，减少模型对特定数据特征的过拟合，从而提高泛化能力。</li>
<li><strong>加速推理</strong>：剪枝后的模型由于参数减少，推理速度得到显著提升。</li>
</ul>
<h4><a id="_287"></a>缺点</h4>
<ul>
<li><strong>需要额外的剪枝步骤</strong></li>
<li><strong>可能影响模型性能</strong>：如果剪枝不当，可能会削弱模型的表现，模型的准确性可能会大幅下降。</li>
<li><strong>需要重新训练</strong>：剪枝后的模型有时需要重新微调或训练，以恢复模型性能。</li>
</ul>
<p>以下是关于常见正则化方法的总结表格：</p>

<table>
<thead>
<tr>
<th><strong>正则化方法</strong></th>
<th><strong>原理</strong></th>
<th><strong>使用场景</strong></th>
<th><strong>优点</strong></th>
<th><strong>缺点</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>L1 正则化</strong> (Lasso)</td>
<td>通过增加权重绝对值惩罚项，实现特征稀疏化，部分权重缩减为 0。</td>
<td>高维稀疏数据集，特征选择任务。</td>
<td>生成稀疏解，易于解释，自动选择重要的特征。</td>
<td>对特征高度相关的数据，可能随机选择特征，导致模型不稳定。</td>
</tr>
<tr>
<td><strong>L2 正则化</strong> (Ridge)</td>
<td>通过增加权重平方和惩罚项，减小权重大小，防止权重过大。</td>
<td>多重共线性问题、回归任务，如岭回归。</td>
<td>防止模型过拟合，处理特征多重共线性问题，模型更加鲁棒。</td>
<td>无法进行特征选择，所有特征权重都被减小。</td>
</tr>
<tr>
<td><strong>Elastic Net 正则化</strong></td>
<td>L1 和 L2 正则化结合，既稀疏化模型，又保留相关性特征。</td>
<td>高维特征的数据集，稀疏化和相关性特征共存的场景。</td>
<td>结合 L1 和 L2 优点，稀疏化与平滑化并存，适用于高维数据。</td>
<td>增加了超参数调节的复杂性。</td>
</tr>
<tr>
<td><strong>Dropout</strong></td>
<td>训练时随机丢弃部分神经元输出，防止神经元对特定特征的依赖，提升泛化能力。</td>
<td>深度神经网络，CNN、RNN，适合复杂模型或数据较少的场景。</td>
<td>有效防止过拟合，提升模型鲁棒性。</td>
<td>训练时间较长，推理时不适用。</td>
</tr>
<tr>
<td><strong>早停法</strong> (Early Stopping)</td>
<td>监控验证集误差，验证集误差不再下降时提前停止训练，防止过拟合。</td>
<td>深度学习模型，梯度下降优化任务，如线性回归、逻辑回归。</td>
<td>简单有效，动态调节训练过程，减少过拟合。</td>
<td>需要合理设置停止条件，可能导致欠拟合。</td>
</tr>
<tr>
<td><strong>Batch Normalization (BN)</strong></td>
<td>对每一批次的输入进行归一化，保持训练过程中的稳定性，并有一定正则化效果。</td>
<td>卷积神经网络和全连接神经网络，适用于大批量训练。</td>
<td>加速训练，减少过拟合，提升模型稳定性。</td>
<td>小批量训练时效果不稳定，增加计算开销。</td>
</tr>
<tr>
<td><strong>权重衰减</strong> (Weight Decay)</td>
<td>在每次权重更新时加入权重衰减项，防止权重过大，等价于 L2 正则化。</td>
<td>大规模神经网络，常与 SGD、AdamW 等优化器配合使用。</td>
<td>简单有效，减少过拟合，类似 L2 正则化。</td>
<td>与 L2 略有不同，某些优化器中的效果不同。</td>
</tr>
<tr>
<td><strong>剪枝</strong> (Pruning)</td>
<td>训练后移除神经网络中不重要的连接或神经元，减少模型规模，降低计算量，提升泛化能力。</td>
<td>移动设备、嵌入式系统、大规模模型压缩，适合资源受限设备和加速任务。</td>
<td>减少模型复杂度，提升推理速度，适合资源受限设备。</td>
<td>需要额外剪枝步骤，可能影响模型性能，需要重新训练。</td>
</tr>
</tbody>
</table><p>这个表格总结了常见的正则化方法，涵盖了其工作原理、使用场景、优点和缺点。根据具体任务和数据集，可以选择合适的正则化方法来提高模型的泛化能力和训练效率。</p>
<hr>
<h1><a id="Reinforcement_Learning_308"></a>强化学习（Reinforcement Learning）</h1>
<h3><a id="1_Q_QLearning_309"></a>1. Q 学习（Q-Learning）</h3>
<p><strong>Q 学习</strong>是一种<strong>基于值函数的强化学习算法</strong>，用于<strong>在离散状态和动作空间中学习最优策略</strong>。它通过<strong>更新 Q 值表来估计状态-动作对的价值，从而指导智能体在环境中的行为</strong>。</p>
<h4><a id="Q__311"></a>Q 学习原理</h4>
<p>Q学习通过Bellman方程更新Q值：</p>
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>=</mo><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>+</mo><mi>α</mi><mo stretchy="false">[</mo><mi>r</mi><mo>+</mo><mi>γ</mi><munder><mrow><mi>max</mi><mo>⁡</mo></mrow><msup><mi>a</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></munder><mi>Q</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><msup><mi>a</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo><mo>−</mo><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">Q(s, a) = Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)] </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right: 0.0278em;">r</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1.5459em; vertical-align: -0.744em;"></span><span class="mord mathnormal" style="margin-right: 0.0556em;">γ</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.4306em;"><span class="" style="top: -2.356em; margin-left: 0em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6828em;"><span class="" style="top: -2.786em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class=""><span class="mop">max</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.744em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8019em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8019em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">a</span><span class="mclose">)]</span></span></span></span></span></span></p>
<p>其中：</p>
<ul>
<li><span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Q(s, a)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">a</span><span class="mclose">)</span></span></span></span></span>为当前状态s执行动作a的价值估计。</li>
<li>α为学习率，控制学习的速度。</li>
<li>γ为折扣因子，控制未来奖励的影响。</li>
<li>r 为当前奖励，<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>s</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">s'</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7519em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7519em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span>为执行动作后到达的下一状态。</li>
</ul>
<h4><a id="_323"></a>应用场景</h4>
<ul>
<li><strong>离散状态和动作空间的决策问题</strong>：如迷宫导航、棋类游戏、路径规划等。</li>
<li><strong>资源管理</strong>：在云计算、通信网络中分配资源。</li>
</ul>
<h4><a id="_326"></a>优缺点</h4>
<ul>
<li><strong>优点</strong>：无需环境模型，能够在部分可观</li>
</ul>
<p>测环境中学习最优策略。</p>
<ul>
<li><strong>缺点</strong>：高维状态空间下需要大量存储，训练时间长。</li>
</ul>
<h3><a id="2__Q_DQN_Deep_Q_Network_331"></a>2. 深度 Q 网络（DQN, Deep Q Network）</h3>
<p><strong>DQN</strong>是结合深度学习和 Q 学习的一种算法，使用神经网络逼近 Q 值函数，能够在高维状态空间中学习有效策略。</p>
<h4><a id="DQN__333"></a>DQN 原理</h4>
<ul>
<li><strong>Q 值逼近</strong>：用神经网络代替传统 Q 学习中的 Q 表，网络输入状态 ( s )，输出各个动作的 Q 值。</li>
<li><strong>经验回放（Experience Replay）</strong>：将智能体的经验存储到回放池中，训练时从中随机抽取小批量经验，打破样本间相关性，提升训练稳定性。</li>
<li><strong>目标网络（Target Network）</strong>：引入目标网络来计算 Q 值更新中的目标值，减少训练过程中的不稳定性。</li>
</ul>
<h4><a id="_338"></a>应用场景</h4>
<ul>
<li><strong>高维状态空间的决策问题</strong>：如 Atari 游戏、无人驾驶、自动驾驶等。</li>
<li><strong>复杂策略学习</strong>：如复杂的游戏 AI、智能交通控制等。</li>
</ul>
<h4><a id="_341"></a>优缺点</h4>
<ul>
<li><strong>优点</strong>：能够处理高维状态空间和复杂策略问题，拓展了 Q 学习的应用范围。</li>
<li><strong>缺点</strong>：训练不稳定，对超参数敏感，训练时间长。</li>
</ul>
<h1><a id="_346"></a>历史文章回顾</h1>
<p><a href="https://blog.csdn.net/haopinglianlian/article/details/143831958?spm=1001.2014.3001.5502">机器学习笔记——损失函数、代价函数和KL散度</a></p>
</div>
</body>

</html>
