
> 大家好，这里是好评笔记，公主号：Goodnote，专栏文章私信限时Free。本笔记介绍机器学习中的感知机、MLP、SVM算法。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/437065b0161744da89945008d2e17a0f.jpeg#pic_center)



>@[toc]

---
# 感知机（Perceptron）

感知机是机器学习中**最基本的线性分类模型之一**，只有**一个输入层**和**一个输出层**，**没有隐藏层**。是一个**简单的线性分类器**，只适用于线性可分的数据集。它最初由 Frank Rosenblatt 于 1957 年提出，**用于解决二分类问题**。感知机的目标是**找到一个能够将两个类别的样本进行线性分割的超平面**（即**线性决策边界**）。

## 模型定义
感知机模型的主要思想是：通过一个**线性函数将输入的特征 x 映射到一个输出类别 y** ，其基本形式如下：

**公式**：

$$f(x) = \text{sign}(w \cdot x + b)$$

- $x$ 是输入的特征向量，表示样本的数据。
- $w$ 是权重向量，表示每个特征的重要性。
- $b$ 是偏置项，它可以**移动分类边界**，使得模型更加灵活。
- $w \cdot x$ 是权重和输入的点积，这个值**决定了输入样本在分类边界的哪一侧**。
- $\text{sign}(\cdot)$ 是符号函数，它将点积的结果映射为 +1 或 -1，用于区分两个类别：
  - 如果 $w \cdot x + b > 0$，则 $f(x) = 1$，即样本属于正类。
  - 如果 $w \cdot x + b < 0$，则 $f(x) = -1$，即样本属于负类。

## 训练过程
感知机的训练过程的核心思想是：**对于每个错误分类的样本，调整权重和偏置**，使其更接近正确分类。（类似神经网络中的**前向传播**和**反向传播**）其学习规则如下：
以下是图片内容的文字提取：

1. **初始化**：将权重 $w$ 和偏置 $b$ 初始化为 0 或随机小值。

2. **遍历数据集**：对训练集中的每个样本 $(x_i, y_i)$，其中 $y_i$ 是样本的真实类别（取值为 1 或 -1）。

3. **判断分类结果**：计算 $y_i(w \cdot x_i + b)$。
   - 如果 $y_i(w \cdot x_i + b) \leq 0$，表示分类错误，需要更新权重和偏置。
   - 如果 $y_i(w \cdot x_i + b) > 0$，表示分类正确，无需更新。

4. **更新规则**（对于错误分类的样本）：
   $$w = w + \eta \cdot y_i \cdot x_i$$
   $$b = b + \eta \cdot y_i$$
   其中，$\eta$ 是学习率，用于控制每次调整的幅度。

5. **重复步骤 2-4**，直到所有样本被正确分类或达到最大迭代次数。
## 优势和局限性
- **优势**：
  - **简单易实现**：感知机是最基础的线性分类模型，理论简单，容易实现。
  - **收敛性保证**：对于**线性可分数据集**，感知机可以保证在有限次迭代内**收敛到一个可以完全分开的超平面**。

- **局限性**：
  - **不适用于线性不可分数据**：感知机只能处理线性可分的数据集，对于线性不可分的数据集，无法给出合适的分类边界。
  - **对噪声敏感**：感知机对异常点和噪声点敏感，可能导致模型过拟合。
  - **无法输出概率**：感知机只能给出硬分类结果（-1 或 1），无法输出分类的概率或置信度。

## 感知机的扩展
感知机模型是很多复杂模型的基础，例如：
- **多层感知机（MLP, Multi-Layer Perceptron）**：由多个感知机堆叠形成，是**神经网络的基础模型**。
- **支持向量机（SVM, Support Vector Machine）**：感知机的扩展版本，能够找到**最大间隔的分类超平面**，适用于**线性不可分**数据的处理。

---
# 多层感知机（MLP, Multilayer Perceptron）
## 定义
多层感知机（MLP）是感知机的扩展版本，它属于**前馈神经网络**，具备**多层结构**，**可以处理更复杂的非线性分类问题**。MLP 是神经网络中最基础的结构，至少包括一个**输入层**、一个或多个**隐藏层**和一个**输出层**。

## 工作原理
MLP 通过多个**神经元层的组合**来学习复杂的非线性关系。每一层的神经元将接收上一层的输出，并通过**激活函数**（如 ReLU、Sigmoid）进行**非线性变换**，然后传递给下一层。输出层则根据最后一层的神经元输出结果作出分类或预测。

**核心概念**：
- **前向传播（Forward Propagation）**：数据从输入层经过隐藏层，逐层计算，最后到达输出层。
- **反向传播（Backpropagation）**：通过计算损失函数的梯度（如交叉熵或均方误差）来调整权重和偏置，更新过程使用梯度下降算法。
MLP 通过多次前向传播和反向传播来优化模型参数，从而在复杂的非线性空间中找到最优的决策边界。

> 前向传播和反向传播参考：[深度学习——**前向传播与反向传播**、神经网络（前馈神经网络与反馈神经网络）、常见算法概要汇总](https://lichuachua.blog.csdn.net/article/details/142515911)
## 优势和局限性
**优势**
- **能处理非线性分类**：MLP 能够通过隐藏层和非线性激活函数**处理复杂的非线性分类问题**。
- **多层结构学习复杂的特征关系**：隐藏层允许 MLP 学习复杂的特征关系，增加模型的表达能力。

**局限性**：
- **训练时间长**：随着网络层数和参数的增加，训练时间也会大幅增加。
- **需要大量数据**：MLP 通常需要大量的训练数据来表现出优势。
- **易过拟合**：在数据不足的情况下，MLP 可能会过拟合训练集，需通过正则化或其他方法来避免过拟合。
---

# 支持向量机（SVM, Support Vector Machine）
## 定义
支持向量机（SVM）是一种用于**分类和回归**的强大算法。SVM 的目标是通过找到一个能够**最大化类间间隔（Margin）的超平面**，将不同类别的样本分开。SVM 可以处理**线性和非线性**分类任务，尤其在**高维数据集上表现良好**。
## 工作原理
SVM 试图找到一个最佳的决策超平面，使得正类和负类之间的间隔最大化。SVM 使用以下两个核心概念：

- **最大间隔超平面**：SVM 寻找使得类别之间的间隔（即最小距离）最大的超平面，增强模型的泛化能力。
- **支持向量**：支持向量是位于**分类边界上的样本点**，它们决定了超平面的最优位置。
## 核函数（Kernel Function）
对于**线性不可分数据**，SVM 引入了**核函数（Kernel Trick）** 进行 **非线性映射** ，将**数据映射到更高维空间**，在该空间中数据变得线性可分。常用的核函数有线性核、多项式核、高斯核（RBF）等。

### 工作原理
核函数的作用是**通核函数`隐式地`计算样本对之间的相似性，将原始的低维数据映射到高维空间，使得在该高维空间中数据可以线性分离**。

直接在高维空间对每个数据点进行计算会非常昂贵，因为每个特征都需要映射到高维空间，而这种映射可能是上千维甚至无限维的，计算量巨大。因此，SVM 使用了**核函数（Kernel Function）**来**避免显式地进行高维映射**。

核函数能够**直接在低维空间中计算两个样本在高维空间中的“相似性”（即高维映射后的点积）**。这就意味着，我们不需要真正把数据点映射到高维空间，而是通过核函数计算结果**隐式地得到**了高维空间中的结果。这一过程称为**核技巧（Kernel Trick）**。

> 核技巧的核心思想是：假设我们有一个**映射函数$\phi(x)$**，它将数据点$x$**从低维空间映射到高维空间**。如果我们想在高维空间中计算两个数据点$x$和$y$的点积$\phi(x) \cdot \phi(y)$，核技巧允许我们直接通过核函数$K(x, y)$来计算，而**不需要显式计算** $\phi(x)$ 和$\phi(y)$。即：
> $$K(x, y) = \phi(x) \cdot \phi(y)$$
> 通过核函数，我们可以直接在低维空间中计算出在高维空间的点积值，避免了高维映射的计算。

### 类型

1. **线性核（Linear Kernel）**

   - **公式**：
     $$K(x_i, x_j) = x_i \cdot x_j$$
   - **含义**：**直接计算两个样本的点积**，适合**数据本身线性可分的情况**。线性核**不进行任何非线性映射**，仅在原始空间中计算样本的内积。
   - **参数**：无特定的参数。
   - **应用场景**：适用于**特征数量大而样本数量少**的数据集，例如文本分类问题中的文档分类。

2. **多项式核（Polynomial Kernel）**

   - **公式**：
$$K(x_i, x_j) = (x_i \cdot x_j + c)^d$$
   - **含义**：通过**引入多项式项**，使得模型能够拟合更复杂的非线性关系。
   - **参数**：
     - **$c$**：常数项，调节映射的偏移。通常为 1，**确保模型的灵活性**。
     - **$d$**：多项式的阶数（degree），**控制映射的复杂性**。阶数越高，模型拟合能力越强，但也**更容易过拟合**。
   - **应用场景**：适合**数据分布非线性**且有较**明显的多项式关系**的数据集。

3. **径向基函数核（Radial Basis Function Kernel, RBF Kernel）或者 高斯核（Gaussian Kernel）**

   - **公式**：
$$K(x_i, x_j) = \exp\left(-\frac{\|x_i - x_j\|^2}{2\sigma^2}\right)$$
     或者写作
$$K(x_i, x_j) = \exp(-\gamma \|x_i - x_j\|^2)$$
   - **含义**：RBF 核**通过高斯分布**将样本映射到**一个无穷维的空间**，能够很好地处理非线性问题。RBF 核的值随着样本之间距离的增加而迅速衰减，因此更关注局部特征。
   - **参数**：
     - **$\sigma$**：带宽参数，**控制样本之间的相似性范围**。较小的 $\sigma$ 值会使核函数更集中于样本附近，更适合细粒度模式；较大的 $\sigma$ 值使核函数范围更广，模型更具全局性。
     - **$\gamma$**：通常定义为 $\gamma = \frac{1}{2\sigma^2}$，因此调节 $\gamma$ 相当于调节 $\sigma$ 的效果。较大的 $\gamma$ 使得模型复杂度增加，容易过拟合；较小的 $\gamma$ 使得模型泛化能力更强。
   - **应用场景**：RBF 核是 SVM 中最常用的核函数之一，**适合大多数非线性数据**，尤其是结构复杂的数据集。

4. **Sigmoid 核（Sigmoid Kernel）**

   - **公式**：
$$K(x_i, x_j) = \tanh(\alpha \cdot x_i \cdot x_j + c)$$
   - **含义**：Sigmoid 核类似于神经网络中的激活函数，使得 SVM 模型的表达能力接近于浅层神经网络。
   - **参数**：
     - **$\alpha$**：比例因子，**控制样本点的内积影响大小**，类似于学习率的作用。
     - **$c$**：偏移量，**控制样本点在 Sigmoid 函数中的位置**。
   - **应用场景**：Sigmoid 核在某种程度上可以模仿神经网络的效果，但在实际应用中**较少用到**，通常在实验时尝试。

### 如何选择
- **线性核**：当数据在原始空间中已经接近线性可分时，线性核是不错的选择，计算量较小，适合高维稀疏数据（如文本分类）。
- **多项式核**：**适合具有多项式关系**的非线性数据，通过调整多项式的阶数来控制模型的复杂度。
- **RBF 核**：是一种通用的核函数，**适合大多数非线性数据集**，可以捕捉复杂的局部特征，是 SVM 中**最常用的核函数**之一（如图像分类）。
- **Sigmoid 核**：类似于神经网络中的激活函数，但实际应用中不常用，通常是实验性地用于测试。

## 软间隔与硬间隔（Soft Margin & Hard Margin）
引入了**软间隔**（soft margin）概念，**引入松弛变量允许一些数据点被错误分类**（通过引入松弛变量），从而能够处理线性不可分的数据。

- **硬间隔（Hard Margin）**：
  - 假设数据是线性可分的，SVM 的目标是找到一个能够将所有样本点完全分开的超平面。
  - 在**硬间隔下**，**所有样本点都满足** $y_i(w \cdot x_i + b) \geq 1$ 的约束。
  - 缺点是**对异常值和噪声数据敏感**，导致模型的泛化能力差。

- **软间隔（Soft Margin）**：
  - 软间隔**允许部分数据点不满足严格的分类条件**，即允许某些样本点可以落入分类间隔中或被误分类。
  - 引入松弛变量 $\xi_i$ 来度量样本的违约程度，优化目标变为：$$\min \frac{1}{2} \| w \|^2 + C \sum_{i=1}^N \xi_i$$
    - 其中，$C$ 是惩罚系数，用于平衡间隔大小和误分类点的数量。
  - 软间隔能够处理数据中的噪声和异常值，具有更好的泛化能力。
## 损失函数
### 硬间隔 SVM 损失函数
硬间隔 SVM 假设数据是**线性可分的**，其目标是找到一个最优超平面，使所有样本点都在分类边界的正确一侧。

**优化目标**：
硬间隔 SVM 的优化目标是最大化间隔，同时确保所有样本点都被正确分类。该目标可以表示为：
$$\min \frac{1}{2} \| w \|^2$$
其中：
- $w$ 是超平面的权重向量（法向量），确定了超平面的方向。

**约束条件**：
对于每个样本 $(x_i, y_i)$，要求满足：
$$y_i (w \cdot x_i + b) \geq 1$$

其中：
- $w$ 是超平面的权重向量（法向量），确定了超平面的方向。
- $b$ 是偏置项。
- $y_i$ 是标签，取值为 +1 或 -1。

硬间隔 SVM 的损失函数只适用于**线性可分的情况**，对噪声和异常值非常敏感，实际应用中并不常用。

### 软间隔 SVM 损失函数（Soft Margin SVM）
引入**软间隔**的概念，以允许部分样本点落入分类间隔中，或者被误分类。软间隔 SVM 引入了**松弛变量** $\xi_i$ 来度量样本的误分类程度。

**优化目标**：
软间隔 SVM 的优化目标是**最大化间隔**，同时**最小化误分类损失**。优化目标为：
$$\min \frac{1}{2} \| w \|^2 + C \sum_{i=1}^N \xi_i$$

其中：
- **$\frac{1}{2} \| w \|^2$**：表示间隔最大化项。
- **$\sum_{i=1}^N \xi_i$**：表示误分类损失项。
- **$C$**：是**惩罚系数**，用于平衡间隔大小和误分类损失之间的权衡。较大的 $C$ 值会使模型更关注误分类损失，容易过拟合；较小的 $C$ 值会让模型更关注间隔大小，具有更好的泛化能力。

**约束条件**：
在软间隔下，每个样本需要满足以下约束条件：
$$y_i (w \cdot x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0$$

其中：
- $\xi_i$ 是松弛变量，表示样本 $x_i$ 的误分类程度。
  - 当 $\xi_i = 0$ 时，样本被正确分类且在间隔之外。
  - 当  $0 < \xi_i \leq 1$ 时，样本被正确分类但在间隔之内。
  - 当 $\xi_i > 1$  时，样本被误分类。

### 合页损失（Hinge Loss）

使用的是**合页损失函数**（hinge loss），并加入了**正则化项**来控制**分类间隔与误分类的权衡**。SVM 的目标是**最小化损失函数**，同时**最大化分类间隔**。
SVM 的损失函数还可以用**合页损失函数**（Hinge Loss）来表示。合页损失是一种常用的损失函数，专门用于最大间隔分类问题。其形式如下：

$$L(y, f(x)) = \max(0, 1 - y \cdot f(x))$$
其中：
- $f(x) = w \cdot x + b$ 表示模型的预测值。
- $y$ 是真实标签，取值为 +1 或 -1。

合页损失的作用是对误分类样本进行惩罚，同时鼓励正确分类样本远离分类边界。合页损失的效果如下：
- 当 $y \cdot f(x) \geq 1$ 时，损失为 0，表示样本被正确分类且位于分类间隔之外。
- 当 $y \cdot f(x) < 1$ 时，损失为 $1 - y \cdot f(x)$，表示样本位于间隔内或被误分类。

因此，软间隔 SVM 的损失函数也可以重写为：
$$\min \frac{1}{2} \| w \|^2 + C \sum_{i=1}^N \max(0, 1 - y_i (w \cdot x_i + b))$$

### 总结
支持向量机的损失函数可以分为硬间隔和软间隔两种：
- **硬间隔**：适用于**线性可分数据**，只关注最大化分类间隔。
- **软间隔**：适用于实际**非线性可分的数据**，通过引入松弛变量允许少量误分类，提高模型的泛化能力。
- **合页损失（Hinge Loss）**：软间隔 SVM 常用的损失函数，**对误分类样本进行惩罚**，并鼓励正确分类样本远离边界。

软间隔 SVM 和合页损失的组合使得 SVM 在处理噪声和异常值方面更具鲁棒性，同时保持良好的分类效果。


## 工作流程
| 步骤                | 详细说明                                                      |
|---------------------|---------------------------------------------------------------|
| **数据准备**        | 收集、整理和预处理数据集，标准化特征。                        |
| **选择核函数**      | 根据数据分布选择合适的核函数，如线性核、多项式核等。          |
| **设置参数**        | 设置正则化参数 \( $C$ \) 和核函数参数（如 $\gamma$）。       |
| **构建模型**        | 通过最大化间隔找到最优超平面。                                |
| **训练模型**        | 通过优化算法求解支持向量、权重和偏置。                        |
| **模型验证与调参**  | 通过交叉验证调整参数，以提升模型性能。                        |
| **模型预测**        | 使用最优超平面对新样本进行分类。                              |
| **评估模型**        | 使用准确率、精确率、召回率等指标评估模型的表现。              |
## SVM多分类
SVM 本身是一个二分类模型，处理多分类问题时，通常使用以下几种策略：
### 一对多（One-vs-Rest, OvR） 
针对**每个类别训练一个 SVM 分类器**，将**该类别样本作为正类，其余所有类别样本作为负类**。
>对于 K 个类别，训练 K 个二分类器，最后选择得分最高的分类器对应的类别作为最终预测结果。

### 一对一（One-vs-One, OvO）
针对每**两个类别组合训练一个 SVM 分类器**。
>对于 $K$ 个类别，需要训练$K(K−1)/2$个二分类器，最终使用投票机制决定类别归属。
>每个二分类器**只使用两个类别的样本**进行训练，将这两个类别中的一个作为正类，另一个作为负类，其余类别的数据不参与该分类器的训练。
>例如，对于一个有 3 个类别（A、B、C）的数据集，OvO 会训练 3 个二分类器：
> - 分类器 1：用类别 A 和 B 的数据训练，将 A 作为正类，B 作为负类。
> - 分类器 2：用类别 A 和 C 的数据训练，将 A 作为正类，C 作为负类。
> - 分类器 3：用类别 B 和 C 的数据训练，将 B 作为正类，C 作为负类。

## 优势与局限性
**优势**：
- **线性和非线性分类**：SVM 能处理线性不可分的数据，通过核函数将数据映射到高维空间进行分类。
- **适合高维数据**：SVM 特别擅长处理高维数据，常用于文本分类、基因数据分析等。
- **稳健性强**：SVM 通过最大化间隔来提高泛化能力，减少过拟合的可能性。

**局限性**：
- **计算复杂度高**：当样本数较大时，SVM 的计算成本较高，尤其是在大规模数据集上训练时，效率较低。
- **核函数选择困难**：SVM 依赖于核函数的选择，不同的数据集需要调试合适的核函数和参数，选择不当可能导致模型表现不佳。


# 对比
## 总结对比
- **感知机（Perceptron）**：适合**线性分类问题**，模型结构简单、计算效率高，但只能处理线性可分的数据。
- **多层感知机（MLP）**：具有更强的表达能力，适合**复杂的非线性任务**，但容易过拟合，训练时间较长，需要大量数据来实现优势。
- **支持向量机（SVM）**：通过最大化分类间隔和使用核函数处理**线性和非线性分类问题**，在**高维小数据集上表现出色**，但计算复杂度较高。

各个模型适合的应用场景不同，选择时应根据具体任务的复杂性、数据规模、非线性程度等进行综合考虑。

以下是**感知机（Perceptron）**、**多层感知机（MLP）** 和 **支持向量机（SVM）** 的详细对比，从多个维度分析它们的区别、优缺点及应用场景。

## 基本概念

| **特性**                 | **感知机（Perceptron）**                               | **多层感知机（MLP）**                            | **支持向量机（SVM）**                             |
|--------------------------|--------------------------------------------------------|-------------------------------------------------|--------------------------------------------------|
| **定义**                 | 最基本的线性二分类模型，使用线性方程区分正类和负类。           | 一种具有多个隐藏层的神经网络，可以处理非线性问题。  | 通过找到最大化间隔的超平面来进行分类，适合线性和非线性分类。 |
| **决策边界**             | 线性分类器。只能找到线性决策边界。                      | 通过多个隐藏层和非线性激活函数找到复杂的非线性决策边界。 | 寻找最大间隔的线性或非线性超平面。                 |

---

## 模型结构

| **特性**                 | **感知机（Perceptron）**                               | **多层感知机（MLP）**                            | **支持向量机（SVM）**                             |
|--------------------------|--------------------------------------------------------|-------------------------------------------------|--------------------------------------------------|
| **层数**                 | 单层结构，只有输入层和输出层。                           | 多层结构，包含输入层、隐藏层和输出层。             | 没有层次结构，通过超平面划分数据。                 |
| **激活函数**             | 无激活函数，仅使用线性分隔。                             | 每层使用非线性激活函数（如 ReLU, Sigmoid）。      | 无激活函数（线性 SVM）；核函数（非线性 SVM）。    |
| **非线性处理**           | 无法处理非线性问题。                                      | 通过隐藏层和非线性激活函数处理复杂的非线性关系。     | 通过核技巧处理非线性问题。                         |

---

## 学习方式和优化

| **特性**                 | **感知机（Perceptron）**                               | **多层感知机（MLP）**                            | **支持向量机（SVM）**                             |
|--------------------------|--------------------------------------------------------|-------------------------------------------------|--------------------------------------------------|
| **学习算法**             | 感知机算法，通过误分类更新权重，简单高效。                | 通过前向传播和反向传播算法训练，使用梯度下降优化。   | 通过凸优化算法，最大化分类间隔，使用 Hinge Loss 和正则化项。 |
| **损失函数**             | 无损失函数，基于误分类更新权重。                          | 常用均方误差或交叉熵损失函数。                      | Hinge Loss（合页损失），主要用于最大化分类间隔。    |
| **参数更新**             | 直接更新权重和偏置，按学习率调整误分类样本的权重。           | 通过反向传播和梯度下降优化所有层的权重。              | 基于支持向量更新模型参数，找到支持向量确定超平面。    |

---

## 适用性和能力

| **特性**                 | **感知机（Perceptron）**                               | **多层感知机（MLP）**                            | **支持向量机（SVM）**                             |
|--------------------------|--------------------------------------------------------|-------------------------------------------------|--------------------------------------------------|
| **适用问题类型**          | 只能处理**线性可分**的二分类问题。                        | 适用于**非线性分类**和回归任务，支持多分类任务。   | 适用于线性和非线性分类任务，也可用于回归。          |
| **对数据的要求**          | 只能处理线性可分数据。                                   | 可以处理复杂数据，适合大规模数据集和高维数据。       | 适合处理小规模且高维的数据，尤其是线性不可分数据。    |
| **模型复杂性**            | 模型简单，计算成本低。                                    | 模型复杂，包含多个隐藏层，计算开销大。              | 复杂度高，尤其在大数据集上训练时，计算开销较大。      |

---

## 优缺点

| **特性**                 | **感知机（Perceptron）**                               | **多层感知机（MLP）**                            | **支持向量机（SVM）**                             |
|--------------------------|--------------------------------------------------------|-------------------------------------------------|--------------------------------------------------|
| **优点**                 | - 简单易实现。<br>- 计算开销低。                         | - 具有较强的表达能力，可处理非线性问题。<br>- 适合处理复杂任务。 | - 具有很强的泛化能力。<br>- 通过核函数处理非线性问题。<br>- 在高维数据中表现良好。 |
| **缺点**                 | - 只能处理线性可分问题。<br>- 不能处理多分类任务。          | - 容易过拟合，需要大量数据来避免过拟合。<br>- 训练时间长。 | - 计算复杂度较高，尤其是在大数据集上训练时。<br>- 对于噪声数据较为敏感。 |
| **过拟合风险**           | 过拟合风险低。                                            | 由于模型复杂，容易过拟合，需要正则化或 Dropout。    | 通过最大化间隔和正则化项减少过拟合的风险。          |

---

## 应用场景

| **特性**                 | **感知机（Perceptron）**                               | **多层感知机（MLP）**                            | **支持向量机（SVM）**                             |
|--------------------------|--------------------------------------------------------|-------------------------------------------------|--------------------------------------------------|
| **常见应用**             | - 简单的二分类问题。                                      | - 图像分类、自然语言处理（NLP）、回归任务。         | - 文本分类、图像识别、基因数据分析、金融数据分析。 |
| **典型使用场景**         | - 小规模、简单的二分类任务。                             | - 需要学习复杂的非线性关系和特征提取的问题。         | - 适合处理高维数据和小样本问题，尤其是线性不可分问题。 |

---

# 历史文章
本系列其他相关笔记参考如下：
[机器学习笔记——损失函数、代价函数和KL散度](https://blog.csdn.net/haopinglianlian/article/details/143831958?)
[机器学习笔记——特征工程、正则化、强化学习](https://blog.csdn.net/haopinglianlian/article/details/143832118?)
[机器学习笔记——30种常见机器学习算法简要汇总](https://blog.csdn.net/haopinglianlian/article/details/143832321)

